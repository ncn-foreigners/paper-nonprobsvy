---
documentclass: jss
author:
  - name: Łukasz Chrostowski
    affiliation: Adam Mickiewicz University
    affiliation2: Pearson
    # use this syntax to add text on several lines
    address: |
      | First line
      | Second line
    email: \email{lukchr@st.amu.edu.pl}
    url: https://posit.co
  - name: Piotr Chlebicki
    affiliation: | 
      | Stockholm University \AND
    address: |
      | Matematiska institutionen
      | Albano hus 1
      | 106 91 Stockholm, Sweden
    email: \email{piotr.chlebicki@math.su.se}
    url: https://github.com/Kertoo, https://www.su.se/profiles/pich3772
    orcid: 0009-0006-4867-7434
  - name: Maciej Beręsewicz
    orcid: 0000-0002-8281-4301
    email: \email{maciej.beresewicz@ue.poznan.pl}
    url: https://github.com/BERENZ, https://ue.poznan.pl/en/people/dr-maciej-beresewicz/
    affiliation: |
      | Poznań University of Economics and Business 
      | Statistical Office in Poznań
    address: |
      |
      | Poznań University of Economics and Business
      | Department of Statistics
      | Institute of Informatics and Quantitative Economics
      | Al. Niepodległosci 10
      | 61-875 Poznań, Poland
      |
      | Statistical Office in Poznań
      | ul. Wojska Polskiego 27/29
      | 60-624 Poznań, Poland
title:
  formatted: "\\pkg{nonprobsvy} -- An R package for modern methods for non-probability surveys"
  # If you use tex in the formatted title, also supply version without
  plain:     "nonprobsvy -- An R package for modern methods for non-probability surveys"
  # For running headers, if needed
  short:     "\\pkg{nonprobsvy} for non-probability surveys"
abstract: >
  The paper presents the \pkg{nonprobsvy} package which implements the state-of-the-art statistical inference methods for non-probability samples. The package implements various approaches that can be categorized into three groups: prediction-based approach, inverse probability weighting and doubly robust approach. On the contrary to the existing packages \pkg{nonprobsvy} assumes existance of either full population or probability-based population information and laverage the \pkg{survey} package for the inference. The package implements both analytical and bootstrap variance estimation for all of the proposed estimators. In the paper we present the theory behind the package, its functionalities and case study that showcases the usage of the package. The package is aimed at official statisticans, public opinion or market researchers who whould like to use non-probability samples (e.g. big data, opt-in web panels, social media) to accurately estimate popualtion characteristics. 
keywords:
  # at least one keyword must be supplied
  formatted: [data integration, doubly robust estimation, propensity score estimation, mass imputation, "\\proglang{R}", "\\proglang{Python}", "\\pkg{survey}"]
  plain: [data integration, doubly robust estimation, propensity score estimation, mass imputation, R, Python, survey]
preamble: >
  \usepackage{amsmath, amsthm, amssymb}
  \usepackage{calc, ragged2e}
  \usepackage[ruled]{algorithm2e}
  \usepackage{algpseudocode}
  \newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
  \newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
  \newcommand{\bX}{\boldsymbol{X}}
  \newcommand{\bx}{\boldsymbol{x}}
  \newcommand{\bY}{\boldsymbol{Y}}
  \newcommand{\by}{\boldsymbol{y}}
  \newcommand{\bh}{\boldsymbol{h}}
  \newcommand{\bH}{\boldsymbol{H}}
  \newcommand{\ba}{\boldsymbol{a}}
  \newcommand{\bp}{\boldsymbol{p}}
  \newcommand{\bA}{\boldsymbol{A}}
  \newcommand{\bw}{\boldsymbol{w}}
  \newcommand{\bd}{\boldsymbol{d}}
  \newcommand{\bZ}{\boldsymbol{Z}}
  \newcommand{\bz}{\boldsymbol{z}}
  \newcommand{\bv}{\boldsymbol{v}}
  \newcommand{\bu}{\boldsymbol{u}}
  \newcommand{\bU}{\boldsymbol{U}}
  \newcommand{\bQ}{\boldsymbol{Q}}
  \newcommand{\bG}{\boldsymbol{G}}
  \newcommand{\HT}{\text{\rm HT}}
  \newcommand{\bbeta}{\boldsymbol{\beta}}
  \newcommand{\balpha}{\boldsymbol{\alpha}}
  \newcommand{\btau}{\boldsymbol{\tau}}
  \newcommand{\bgamma}{\boldsymbol{\gamma}}
  \newcommand{\btheta}{\boldsymbol{\theta}}
  \newcommand{\blambda}{\boldsymbol{\lambda}}
  \newcommand{\bPhi}{\boldsymbol{\Phi}}
  \newcommand{\bEta}{\boldsymbol{\eta}}
  \newcommand{\bZero}{\boldsymbol{0}}
  \newcommand{\colvec}{\operatorname{colvec}}
  \newcommand{\logit}{\operatorname{logit}}
  \newcommand{\Exp}{\operatorname{Exp}}
  \newcommand{\Ber}{\operatorname{Bernoulli}}
  \newcommand{\Uni}{\operatorname{Uniform}}
output:
  rticles::jss_article:
    number_sections: TRUE
    citation_package: natbib
bibliography: references.bib
biblio-style: jss
---

```{r, setup, include=FALSE}
options(prompt = 'R> ', continue = '+ ')
```

# Introduction {#sec-introduction}

In official statistics, information about the target population and its characteristics is mainly collected through probability surveys, census or is obtained from administrative registers, which covers all (or nearly all) units of the population. However, owing to increasing non-response rates, particularly unit non-response and non-contact, resulting from the growing respondent burden, as well as rising costs of surveys conducted by National Statistical Institutes (NSIs), non-probability data sources are becoming more popular \citep{berkesewicz2017two, beaumont2020probability, biffignandi2021handbook}. Non-probability surveys, such as opt-in web panels, social media, scanner data, mobile phone data or voluntary register data, are currently being explored for use in the production of official statistics \citep{citro2014multiple,daas2015big}, public opinion studies or market research. Since the selection mechanism in these sources is unknown, standard design-based inference methods cannot be directly applied and in case of large datasets may lead to \textit{big data paradox} as described by \citet{meng2018statistical}.

Table \ref{tab-comparison-characteristics} compares the basic characteristics of probability and non-probability samples. In particular, what are the advantages and disadvantages of each type of sample with respect to population coverage, bias, variance, costs, and the selection mechanism for observations into the samples. In general, non-probability samples suffers from unknown selection mechanism (i.e. unknown probabilities of inclusion into sample) and under-coverage of certain groups from the population. As a result direct estimation based on these samples are characterised with bias and, in most cases, small variance (for large non-probability surveys) which leads to so called big data paradox. Certainly, cost and timeliness of these surveys is significantly smaller than for non-probability samples.

\begin{table}[ht!]
    \centering
    \begin{tabular}{lll}
    \hline
    \textbf{Factor}   &  \textbf{Probability sample} & \textbf{Non-probability sample}\\
    \hline
    Selection & Known probabilities & Unknown self-selection \\
    Coverage & Complete & Incomplete \\
    Estimation bias & Unbiased under design & Potential systematic bias \\
    Variance of estimates & Typically high & Typically low \\
    Cost & High & Low \\
    Availability & Long & Rapid \\
    \hline
    \end{tabular}
    \caption{Comparison of probability and non-probability samples and its characteristics}
    \label{tab-comparison-characteristics}
\end{table}

To address this problem, several approaches based on inverse probability weighting (IPW; also propensity score weighting/adjustment, \citet{lee2006propensity}), model-based prediction (in particular, mass imputation estimators; MI) and doubly robust (DR) approach have been proposed for two main scenarios: 1) population-level data are available, either in the form of unit-level data (e.g. from a register covering the whole population) or known population totals/means, and 2) only survey data is available as a source of information about the target population \citep[cf.][]{elliott_inference_2017}. \citet{wu2022statistical} classified these approaches into three groups that require a joint randomization framework involving \textit{probability sampling design} (denoted as $p$) and one of the outcome regression model (denoted as $\xi$) or propensity score model (denoted as $q$). In this approach the IPW estimators are under the $qp$ framework, the MI estimators are under the $\xi p$ framework, DR estimators are under the $qp$ or $\xi p$ framework.

Most approaches assume that population data are used to reduce the bias of non-probability sampling by a proper reweighting to reproduce known population totals/means (i.e. IPW estimators); by modelling target variable using various techniques (i.e. MI estimators); or combining both approaches (for instance DR estimators, cf. \citet{chen2020doubly}; Multilevel Regression and Post-stratification (MRP) also called \textit{Mister-P}, cf. \citet{gelman1997poststratification}). This topic have became very popular and number of new methods were proposed; for instance non-parametric approaches based on nearest neighbours \citep{yang2021integration}, kernel density estimation \citep{chen_nonparametric_2022}, empirical likelihood \citep{kim2023empirical} or quantile balanced \citep{beresewicz2025} to name few. It should be highlighted that, on contrary to probability samples, there is no single method that can be used for non-probability samples. As shown literature, and thus statistical software, offers various methods as presented in the next section.

## Software for non-probability samples {#sec-software}

Table \ref{tab-comparisons} presents comparison of availability of various inference methods and functionalities of selected packages. We focused on packages available through CRAN or PyPI (for non-CRAN or non-PyPI software see \citet{cobo2024software}).

In the comparison we have included four packages that particularly focus on non-probability samples: \pkg{NonProbEst} \citep{NonProbEst}, \pkg{balance} \citep{sarig2023balancepythonpackage}, \pkg{inps} \citep{castro2024inps} and our \pkg{nonprobsvy}. We have included two packages that implements specific methods: \pkg{rstanarm} (MRP; \citet{rstanarm}) and \pkg{GJRM} (generalized sample selection models; \citet{GJRM}). 

\begin{table}[ht!]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{p{4cm}cccccc}
\hline
\textbf{Functionalities} & \pkg{NonProbEst} & \pkg{rstanarm} & \pkg{GJRM} & \pkg{balance} & \pkg{inps} & \pkg{nonprobsvy} \\
\hline
IPW  & $\checkmark$ & -- & +/- & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
Calibrated IPW  & -- & -- & -- & -- & -- & $\checkmark$ \\
MI & $\checkmark$ & -- & -- & -- & -- & $\checkmark$ \\
DR & -- & -- & -- & +/- & $\checkmark$ & $\checkmark$\\
MRP & -- & $\checkmark$ & -- & --  &--  & -- \\
Sample selection & -- & -- & $\checkmark$ & --  & --  & -- \\
Variable selection & $\checkmark$ & -- & -- & $\checkmark$ & -- & $\checkmark$\\
Analytical variance & -- & -- & -- & -- & -- & $\checkmark$\\
Bootstrap variance & $\checkmark$ & -- & -- & -- & -- & $\checkmark$\\
Integration with \pkg{survey} or \pkg{samplics} & -- & -- & -- & -- & -- & $\checkmark$\\
\hline
\end{tabular}
}
\caption{Comparison of packages and implemented methods}
\label{tab-comparisons}
\end{table}

The \pkg{NonProbEst} implements IPW under logistic regression as well as machine learning approaches (e.g. CART, GBM or Neural Networks) however does not provide theoretical justification of using the ML approaches. The \pkg{balance} focuses solely on the IPW and uses standard and covariate balancing propensity score.

It should be however noted that implementation of specific approaches may vary significantly as the documentation of specific modules is limited. For instance, \pkg{NonProbEst} allows for calibration of IPW weights \textit{after} they are estimated, while \pkg{nonprobsvy} does this in one step. On the other hand \pkg{NonProbEst} implements various weights based on the estimated propensity scores while  \pkg{nonprobsvy} supports only two types of IPW weights. Furthermore, it should be noted that \pkg{NonProbEst}, \pkg{balance} and \pkg{inps} implements survey calibration for non-probability sample but it is not theoretically justified as the inclusion probabilities are unknown (on the contrary to probability samples). Only the \pkg{nonprobsvy} package leveraged the use of the \pkg{survey} package to estimate analytic as well as bootstrap variance of the non-probability estimators. 

<!--- 
Dodać informację jakie metody są oprogramowane z jakich papierów
dodać informację, że te możliwa jest kombinacja różnych metod
--->

To our knowledge the \pkg{nonprobsvy} is the solely software (open or close) that implements state-of-the-art methods for non-probability samples.

The remaining part of the paper is as follows. In Section \ref{sec-methods} theory of the statistical inference based on non-probability samples is presented. We provide basic set-up and introduce methods in separate subsections. In the paper we @wu2022statistical notation. Section \ref{sec-package} describes the dependencies of the package and the main function with its arguments. Section \ref{sec-data-analysis} presents a case study of integration of the Polish Job Vacancy Survey with a voluntary admin data: Central Job Offers Database with an aim on estimating number of companies with at least vacancy offered on a single shift. Section \ref{sec-s3methods} presents classes and the \code{S3Methods} implemented in the package. Paper finishes with summary and plans for the future works. In the Appendix we present codes for specific approaches discussed in the paper, algorithms and detailed derivations of the implemented methods.

# Methods for non-probability samples {#sec-methods}

## Basic setup

Let $U=\{1,..., N\}$ denote the target population consisting of $N$ labelled units. Each unit $i$ has an associated vector of auxiliary variables $\bx_{i}$ and the study/target variable $y_{i}$. Let $\{ (y_i, \bx_i), i \in S_A\}$ be a dataset of a non-probability sample $S_A$ of size $n_A$ and let  $\{\left(\bx_i, \pi_{i}^B\right), i \in S_B\}$ be a dataset of a probability sample $S_B$ of size $n_B$, where only information about variables $\bX$ and inclusion probabilities $\pi^B$ are known for all units in the population. Each unit in the sample $S_B$ has been assigned a design-based weight given by $d_i^B = 1/\pi_i^B$. 

Let $R_i=I(i \in S_A)$ be an indicator of inclusion into non-probability sample $S_A$ defined for all units in the target population. Let $\pi_i^A=P(R_i=1 \mid \bx_i, y_i)=P(R_i=1 \mid \bx_i)$ be the propensity scores which characterize the sample $S_A$ inclusion and participation mechanisms. On the contrary to $\pi_i^B$,  the $\pi_i^A$ and thus $d_i^A=1/\pi_i^A$ are unknown. This description of the data is presented in a more concise form in Table \ref{tab-two-sources}.

\begin{table}[ht!]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llcccc} 
    \hline
    Sample & ID & Inclusion ($R$) & Design weight ($d$) & Covariates ($\bX$) & Study variable ($Y$) \\
    \hline
    Non-probability  & 1 & 1 & ? & $\checkmark$ & $\checkmark$ \\ 
    sample ($S_A$) & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
    & $n_A$ & 1 & ? & $\checkmark$ & $\checkmark$ \\
    Probability  & 1 & 0 & $\checkmark$ & $\checkmark$ & ? \\
    sample ($S_B$) & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\ 
    & $n_B$ & 0 & $\checkmark$ & $\checkmark$ & ? \\                                     
    \hline     
    \end{tabular}
    }
    \caption{Two sample setting}
    \label{tab-two-sources}
\end{table}

The goal is to estimate a finite population mean $\displaystyle\mu_{y}=\frac{1}{N}\sum_{i=1}^{N} y_{i}$ of the target variable $Y$. As values of $y_{i}$ are not observed in the probability sample, it cannot be used to estimate the target quantity. Instead, one could try combining the non-probability and probability samples to estimate $\mu_{y}$. Given the absence of a universally accepted methodology for achieving this objective, the assumptions vary considerably, as outlined by @wu2022statistical. However, the main assumptions that apply to all presented in this section methods are: 

\begin{itemize}
\item[A1] $R_i$ and the study variable $y_i$ are independent given the set of covariates $\bx_i$, i.e., $\left(R_i \perp y_i\right) \mid \bx_i$.
\item[A2] All the units in the target population have non-zero propensity scores, i.e., $\pi_i^A>0$, $i=1,2, \ldots, N$.
\item[A3] The indicator variables $R_1, R_2, \ldots, R_N$ are independent given the set of auxiliary variables $\left(\bx_1, \bx_2, \ldots, \bx_N\right)$.
\end{itemize}

In addition, we assume no overlap between $S_A$ and $S_B$, and no measurement error in $Y_i$ and $\bX_i$ is observed. Setting presented in Table \ref{tab-two-sources} may be also extended to calibrated $d_i^B$ weights (i.e. $d_i^B$ adjusted for under-coverage, non-contact or non-response; cf. \citet{sarndal2005estimation}) but this requires additional developments in the theory about the consistency of the IPW, MI or DR estimators. In the next sections we briefly present methods that are implemented in the package.

## Prediction-based approach {#sec-prediction}

As @wu2022statistical notes there are two commonly used prediction estimators

\begin{equation}
\hat{\mu}_{y 1}=\frac{1}{N} \sum_{i=1}^N \hat{m}_i \quad \text { and } \quad \hat{\mu}_{y 2}=\frac{1}{N}\left\{\sum_{i \in S_A} y_i-\sum_{i \in S_A} \hat{m}_i+\sum_{i=1}^N \hat{m}_i\right\} .
\end{equation}


Mass imputation is the application of imputation techniques to an entire dataset where many observations have missing values for the given variable. \cite{kim_combining_2021}, \cite{yang2021integration}, \cite{Beres} propose the following imputation strategies as:

\begin{itemize}
    \item Model based approach (GLM),
    \item Nearest neighbour imputation (NN),
    \item Predictive mean matching (PMM).
\end{itemize}

Mass imputation is particularly useful in large datasets where missing data can be widespread, and it seeks to preserve the relationships between variables, thus improving the overall integrity of the data.

As presented in Table \ref{tab-two-sources}, we do not know the value of the dependent variable $Y$ for the units in the probability sample. In this case, the method will be to impute the values of the explanatory variable for all units in the probability sample. We therefore treat the non-probability sample as a training set that is used to build the imputation model. In this subsection, we distinguish three main methods of mass imputation based on linear models and the k-nearest neighbours algorithm. Other popular methods for estimating the variable $Y$ from the variable $\bX$ can also be considered, e.g. machine learning models such as random forests or neural networks. 

We can obtain an estimate of the population mean based on known design weights and an imputation model for units from the probability sample:

\begin{equation}
\hat{\mu}_{M I}=\frac{1}{\hat{N}_{\mathrm{B}}} \sum_{i \in S_{\mathrm{B}}} d_i^{\mathrm{B}} \hat{y}, 
\end{equation}
 $\hat{N}_{\mathrm{B}} = \sum_{i \in S_B} d_i^B$ and $\hat{y}$ is the estimated value of $y$ for units from probability samples based on mass imputation model.
 
  This estimator can be understood as a version of the Horvitz--Thompson estimator, which are used to estimate mean or total values in the population (based on probability sampling and inclusion probabilities). The only difference is that in our case, instead of the known values of the $Y$ variable, we use its estimated equivalents.
  
### Generalized Linear Models

Let us assume the following parametric model for the sample $S_A$ based on the conditional expected value of the variable $Y$. Let
\begin{equation}
\mathbb{E}\left(y_i \mid \boldsymbol{x}_i\right)=m\left(\boldsymbol{x}_i, \boldsymbol{\beta}_0\right)
\end{equation}
for a certain $p$--dimensional vector $\boldsymbol{\beta}_0$ and a known $m$ function from a given class of mean functions for generalized linear models.

According to the model described, we have 
$$
y_i=m\left(\boldsymbol{x}_i\right)+\varepsilon_i, \quad i=1,2, \ldots, N.
$$
We also assume that the random variables $\varepsilon_i$ are independent with $\mathbb{E} \left(\varepsilon_i \right) = 0$ and $\sigma^2 \left(\varepsilon_i \right) = \mathbf{v} \left(\bx\right) \sigma^2$. It is assumed that $\mathbf{v} \left(\bx\right)$ has a known value and is homogeneous, i.e. homogeneous, regardless of the sample under study. Let us represent the process of mass imputation of a linear model to a sample $S_B$. Finally we are interested in finding a vector $\bbeta$ solves the following equation:
\begin{equation}
\label{eq-2.3}
U(\boldsymbol{\beta})=\frac{1}{n_A} \sum_{i \in S_A}\left\{y_i-m\left(\bx_i ; \boldsymbol{\beta}\right)\right\} h\left(\bx_i ; \boldsymbol{\beta}\right)=\mathbf{0},
\end{equation}

for some $p$-dimensional vector of function $h\left(\bx_i ; \boldsymbol{\beta}\right)$, where $h\left(\bx_i ; \boldsymbol{\beta}\right) = \bx_i$ might be used for certain applications. The mass imputation process is described in Algorithm \ref{algo-1}.


### Nearest Neighbour Algorithm

On the other hand, it is also possible to consider a non-parametric model for the problem described, i.e. for each individual from sample $S_B$, the k-nearest neighbours from sample $S_A$ are found based on the values of the auxiliary vector $\bX$ and the corresponding metric. Then, the missing values of the variable $Y$ from sample $S_B$ are replaced by the values (or their mean if more than one neighbour is considered) of this variable for the corresponding neighbours from sample $S_A$. The algorithm is as follows

Note that the algorithm differs depending on the number of nearest neighbours chosen. In case $k=1$ the nearest neighbour value is imputed according to the chosen metric, for example the Euclidean metric. In case $k>2$ the average of the nearest neighbours values is imputed. The literature indicates that this method suffers from the so-called curse of multidimensionality, i.e. for samples with several explanatory variables, imputation can lead to a large variance in the estimator. On the other hand, the algorithm is easy to interpret and simple to implement. 

### Predictive Mean Matching

Predictive mean-matching imputation is a particularly well-known way of dealing with non-response among respondents, and is favoured by statistical offices for compiling a country's official population statistics. It is a version of the k-nearest neighbour algorithm, but instead of looking at the distances between the vectors of the auxiliary variables, it looks at the distance between the functions of the mean vectors. This helps to reduce the curse of multidimensionality and, at the same time, allows the observed values of the explanatory variable or their mean to be calculated.
Let us therefore present two algorithms that describe the steps to follow to perform a mass imputation using the mean matching method.


<!--- Piotrek, 17.11.2024:
Tutaj właśnie najlepiej tylko rozbudować to co jest poniżej i wywalić te algorytmy.
-->

As can be seen, the difference between the two algorithms is due to step 2. In the first approach, we compare $\hat{y}$ from samples $S_A$ and $S_B$. The second, on the other hand, compares $\hat{y}$ from sample $S_B$ with the known $y$ from sample $S_A$. It is worth noting that proof of the consistency of these estimators can be found in \citet{Beres}.

## Inverse Probability Weighting {#sec-ipw}

The main disadvantage of non-probability sampling is the unknown selection mechanism for a unit to be included in the sample. This is why we talk about the so-called biased sample problem. The inverse probability approach is based on the assumption that a reference probability sample is available and therefore we can estimate the propensity score of the selection mechanism. In recent years, a number of articles have addressed this issue. \cite{chen2020doubly} propose maximum likelihood estimation approach for estimating propensity scores for selection mechanism. \cite{wu2022statistical} present the approach based on generalized estimating equations, this method is also mentioned in \cite{yang_doubly_2020}. On the other hand calibration approach for quantiles was explained \cite{beresewicz2024inference} and \cite{santanna_covariate_2022} present the approach based on maximize the covariate distribution balance among different treatment groups.

In the formal framework, let us introduce the following assumptions for propensity score model, which will imply a number of properties derived in the thesis.
\begin{itemize}
    \item[(A1)] The selection indicator $R_i^A$ and explanatory variable $y_i$ are independent.
    \item[(A2)]All units have a so-called non-probability sample propensity score, which is non-zero, i.e. $\pi_i^{\mathrm{A}} > 0$, where $\pi_i^{\mathrm{A}} = P_q\left(R_i^A=1 \mid \bx_i, y_i\right)$, where $q$ refers to the model for the selection mechanism for the non-probability sample (propensity score model).
    \item[(A3)] Indicator variables $R_i^A$ and $R_j^A$ are independent with $i \neq j$. 
\end{itemize}

The estimated propensity score is used to construct an inverse probability weighting estimator of the population mean of the form

\begin{equation}
\begin{gathered}
\hat{\mu}_{I P W}=\frac{1}{\hat{N}^A} \sum_{i \in S_A} \frac{y_i}{\hat{\pi}_i^A}.
\end{gathered}
\end{equation}
where $\hat{N}^A = \sum_{i \in S_A} \hat{d}_i^A = \sum_{i \in S_A} \frac{1}{\hat{\pi}_i^A}$.

### Maximum Likelihood Estimation

Consider the following likelihood function
\begin{align}
    \begin{split}
 \ell(\boldsymbol{\theta}) & =\sum_{i=1}^N\left\{R_i^A \log \pi_i^{\mathrm{A}}+\left(1-R_i^A\right) \log \left(1-\pi_i^{\mathrm{A}}\right)\right\} \\ & =\sum_{i \in S_{\mathrm{A}}} \log \left\{\frac{\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)}{1-\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)}\right\}+\sum_{i=1}^N \log \left\{1-\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)\right\}
    \end{split}
\end{align}
In practice, a function of this form cannot be used because we do not observe all units from the population. Hence, the second component of the function is replaced by the Horvitz-Thompson estimator, which is used when having access to the design weights for the units in the sample. In our case, these will be the weights $d_i^B$ for the units in the sample $S_B$. 
We then have
\begin{equation}
\ell^*(\boldsymbol{\theta})=\sum_{i \in S_{\mathrm{A}}} \log \left\{\frac{\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)}{1-\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)}\right\}+\sum_{i \in S_{\mathrm{B}}} d_i^{\mathrm{B}} \log \left\{1-\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)\right\}.
\end{equation}
Our objective is to find the maximum likelihood estimator $\hat{\pi}_{i}^{A} = \pi(\bx_{i}, \hat{\btheta})$, such that $\hat{\btheta}$ maximises the function defined above.

### Generalized Estimating Equations {#sec-ipw-gee}

Equations of the type $\operatorname{U}(\btheta) = \bZero$, where $\operatorname{U}(\btheta) = \frac{\partial}{\partial \btheta} l^*(\btheta)$, obtained from the maximum likelihood estimation can be replaced by a system of generalized estimating equations of the form
\begin{equation}
\label{gee}
\mathbf{G}(\btheta)=\sum_{i \in S_A} h\left(\bx_i, \btheta\right)-\sum_{i \in S_B} d_i^B \pi\left(\bx_i, \btheta\right) h\left(\bx_i, \btheta\right) = \bZero,
\end{equation}
where $h\left(\bx_i, \btheta\right)$ is a certain continuous function. In the literature, the most commonly considered functions are $h\left(\bx_i, \btheta\right) = \bx_i$ and $h\left(\bx_i, \btheta\right) = \bx_i \pi\left(\bx_i, \btheta\right)^{-1}$. Note that if the function $h$ is equal to the vector of observed characteristics $\bx$, then $\mathbf{G}$ is reduced to
$$
\mathbf{G}(\btheta) = \sum_{i \in S_A} \boldsymbol{x}_i-\sum_{i \in S_B} d_i^B \pi\left(\bx_i, \boldsymbol{\theta}\right) \boldsymbol{x}_i.
$$
In the next subsection we will proof that this is disorted version of 
MLE approach with $\pi_i^A$ modelling by logistic regression.
If we use the second form of the function $h$ we get the following form of the function $\mathbf{G}$
$$
\mathbf{G}(\btheta) = \sum_{i \in S_A} \frac{\boldsymbol{x}_i}{\pi\left(\bx_i, \boldsymbol{\theta}\right) }-\sum_{i \in S_B} d_i^B \boldsymbol{x}_i.
$$

The advantage of this method is the ability to estimate with global values of the variables (e.g. from external sources) instead of a probability sample. Note that this is allowed by the second form of the $\mathbf{G}$ function. Its second term is nothing more than the estimated sums of the $\bx$ variables. On the other hand, empirical studies suggest that the process of solving this type of equation may be less stable than the maximum likelihood method. In other words, the iterative algorithm for finding zeros that satisfy equation (\ref{gee}) may not converge. 

<!--- Piotrek, 17.11.2024:
Imo nie trzeba tłumaczyć czym jest logit, probit, cloglog. To można nawet na wikipedi znaleźć.
Jak się logit, probit, cloglog tłumaczy na statystyke? Który lepiej wybrać w jakiej sytuacji?
Są jakieś heurystyki czy nikt o tym nie pisał? To jest bardziej istotne.
-->

In the `nonprobsvy` package the propensity scores can modelled using three different link functions: logistic, complementary log-log and probit. The logistic regression is the most commonly used for modelling probabilities. This method is based on the so-called sigmoidal function and for our settign it has the form
It satisfies certain properties to model the probability. For our scheme, this will be the probability of belonging to the non-probability sample. Another approach to modelling binary variables and also probabilities is probit regression. It is based on the standard normal distribution and as logit is also symmetric around $p = 0.5$. On the other hand regression with the cloglog model is particularly useful if we are modelling rare phenomena, i.e. the probabilities will oscillate around the values 1 and 0. Compared to the sigmoidal function and the distribution, the cloglog function is more asymmetric towards the value 0.5.


## Doubly Robust approach {#sec-dr-approach}

The inverse probability weighting and mass imputation estimators are sensible on misspecified models for propensity score and outcome variable respectively. For this purpose so called doubly-robust methods, which take into account these problems, are presented.

The proposed estimation procedure addresses the challenge of combining data from nonprobability and probability survey samples. Traditional semiparametric models, often applied to such problems, are not directly usable in this context due to the distinct nature of the two samples. Instead, a joint randomization framework is employed, integrating semiparametric models for propensity scores with outcome regression for the nonprobability sample and design-based inference from the probability sample. This framework leads to a doubly robust (DR) estimation approach, which is effective in the presence of model misspecifications.

Inverse Probability Weighted (IPW) estimators are sensitive to misspecified propensity score models, particularly when propensity scores are very small. To improve robustness and efficiency, the doubly robust method incorporates a prediction model for the response variable. Moreover, even if one of the models is misspecified, the DR estimator remains consistent, showcasing the ``double robustness'' property.

### Joint Randomization Approach

The joint randomization approach combines two processes: the selection mechanism of a non-probability sample, modelled by propensity scores, and the design-based inference from a probability sample.

The response $y_i$ is predicted using a regression model $m(\bx_i, \bbeta)$ (or NN/PMM methods), where $\bbeta$ is estimated from the non-probability sample. With known design weights $d_i^B$ for $i \in S_B$ we can define the DR estimator as
\begin{equation}
\label{dr}
\hat{\mu}_{\mathrm{DR}}=\frac{1}{\hat{N}^{\mathrm{A}}} \sum_{i \in S_{\mathrm{A}}} d_i^{\mathrm{A}}\left\{y_i-m\left(\boldsymbol{x}_i, \hat{\boldsymbol{\beta}}\right)\right\}+\frac{1}{\hat{N}^{\mathrm{B}}} \sum_{i \in S_{\mathrm{B}}} d_i^{\mathrm{B}} m\left(\boldsymbol{x}_i, \hat{\boldsymbol{\beta}}\right),
\end{equation}
where $d_i^A=\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)^{-1}, \hat{N}^A=\sum_{i \in S_A} d_i^A$ and $\hat{N}^B=\sum_{i \in S_B} d_i^B$.

It remains consistent if either the propensity score model $\pi(\bx_i, \btheta)$ or the outcome regression model $m(\bx_i, \bbeta)$ is correctly specified.

The joint randomization approach ensures robustness by accounting for randomness in both the non-probability sample through $\pi(\bx_i, \btheta)$ and the probability sample through design-based inference.

### Minimization of the bias for doubly robust methods

By reducing the variance of the estimators, for example by variable selection, we cannot control the bias of the estimator, which may increase. Therefore, according to @yang_doubly_2020, the idea is to determine the equations leading to the estimation of the $\bbeta$ and $\btheta$ parameters based on the bias of the population mean estimator. In contrast to the joint randomization approach, this method allows for the estimation of the parameters $\boldsymbol{\beta}$ and $\boldsymbol{\theta}$ in a single step, rather than in two separate steps.

We will first present the bias of the doubly robust estimator and then, using optimisation techniques, discuss the equations leading to its minimization. Thus we have
\begin{equation}
\begin{aligned}
\operatorname{bias}\left(\hat{\mu}_{D R}\right) & = \mid\hat{\mu}_{DR}-\mu\mid& \\
& =\frac{1}{N} \sum_{i=1}^N\left\{\frac{R_i^A}{\pi_i^A\left(\boldsymbol{x}_i^{\mathrm{T}} \boldsymbol{\theta}\right)}-1\right\}\left\{y_i-m\left(\boldsymbol{x}_i^{\mathrm{T}} \boldsymbol{\beta}\right)\right\}\\
& + \frac{1}{N} \sum_{i=1}^N\left(R_i^B d_i^B-1\right) m\left(\boldsymbol{x}_i^{\mathrm{T}} \boldsymbol{\beta}\right)
\end{aligned}
\end{equation}

To minimize $\operatorname{bias}\left(\hat{\mu}_{D R}\right)^2$ let us calculate the gradient of the square of the bias at $\left(\bbeta, \btheta\right)$. We then have 

\begin{equation*}
\frac{\partial \operatorname{bias}\left(\hat{\mu}_{D R}\right)^2}{\partial\left(\boldsymbol{\beta}^{\mathrm{T}}, \boldsymbol{\theta}^{\mathrm{T}}\right)^{\mathrm{T}}}=2 \operatorname{bias}\left(\hat{\mu}_{D R}\right) J(\theta, \beta),
\end{equation*}
where
\begin{equation*}
J(\theta, \beta)=\left(\begin{array}{l}
J_1(\theta, \beta) \\
J_2(\theta, \beta)
\end{array}\right)=\left(\begin{array}{c}
\sum_{i=1}^N R_i^A\left\{\frac{1}{\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)}-1\right\}\left\{y_i-m\left(\boldsymbol{x}_i, \boldsymbol{\beta}\right)\right\} \boldsymbol{x}_i \\
\sum_{i=1}^N \frac{R_i^A}{\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)} \frac{\partial m\left(\boldsymbol{x}_i, \boldsymbol{\beta}\right)}{\partial \boldsymbol{\beta}}-\sum_{i \in S_{\mathrm{B}}} d_i^{\mathrm{B}} \frac{\partial m\left(\boldsymbol{x}_i, \boldsymbol{\beta}\right)}{\partial \boldsymbol{\beta}}
\end{array}\right),
\end{equation*}
which leads to the problem of solving the following system of equations
\begin{equation}
\label{bias-min}
    \left(\begin{array}{c}
\sum_{i=1}^N R_i^A\left\{\frac{1}{\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)}-1\right\}\left\{y_i-m\left(\boldsymbol{x}_i, \boldsymbol{\beta}\right)\right\} \boldsymbol{x}_i \\
\sum_{i=1}^N \frac{R_i^A}{\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)} \frac{\partial m\left(\boldsymbol{x}_i, \boldsymbol{\beta}\right)}{\partial \boldsymbol{\beta}}-\sum_{i \in S_{\mathrm{B}}} d_i^{\mathrm{B}} \frac{\partial m\left(\boldsymbol{x}_i, \boldsymbol{\beta}\right)}{\partial \boldsymbol{\beta}}
\end{array}\right) = \bZero,
\end{equation}
which can be solved using Newton--Raphson optimization method.

## Variable selection algorithms {#sec-varsel}

When dealing with multivariate data with a large number of features, it is recommended to select variables for estimation that are statistically significant for the model under consideration. \citet{yang_asymptotic_2020} point that using variable selection techniques during estimation is crucial, especially when dealing with high-dimensional data. Variable selection not only improves model stability and computational feasibility, but also reduces variance, which can increase when irrelevant auxiliary variables are included. Including irrelevant variables increases the complexity of the model and makes the estimation process more error-prone and unstable. Therefore, variable selection is key to ensure robust and efficient estimation. Very popular in the statistical literature are variable selection methods such as \textit{Least Absolute Shrinkage and Selection Operator} (\textbf{LASSO}), \textit{Smoothly Clipped Absolute Deviation} (\textbf{SCAD}) or \textit{Minimax Concave Penalty} (\textbf{MCP}), which, thanks to appropriate loss functions, degenerate the coefficients in variables that have no significant effect on the dependent variable. In this way, the result obtained, for example, a linear regression equation, is based only on the features selected by the model. The selection procedure works in a similar way for non-probability methods using external data sources such as sample or population totals or averages. In particular, the technique is divided into two steps. In the first, we select the relevant variables using an appropriately constructed loss function and the estimating equations used. In the second case, we construct the equations on the basis of the derived biases of the relevant estimator, whose value we calculate only on the basis of the selected characteristics.

In the case of data integration based on probability and nonprobability samples, the selection of variables is part of a two-step process leading to the estimation of the mean, where in the first step statistically significant variables are selected and in the second step the model is rebuilt. For the first step, a penalized logistic regression model has been proposed to estimate propensity scores (\citet{yang_doubly_2020}), but this approach can be extended to other linking models such as clog-log and probit functions. For a parametric model-based mass imputation, penalized OLS (\textit{Ordinary Least Squared}) is considered. It is worth mentioning that \citet{yang_asymptotic_2020}, in their article on this topic, used the SCAD method (\textit{Smoothly Clipped Absolute Deviation}), but we extend it on other selection techniques such as LASSO and MCP.


# The main function {#sec-package}

\subsection[The nonprob function]{The \code{nonprob} function}

The \pkg{nonprobsvy} package is built around the \code{nonprob} function. The main design objective was to make using \code{nonprob} as similar as possible to standard R functions for fitting statistical models, such as \code{stats::glm}, while incorporating survey design features from the \pkg{survey} package. The most important arguments are given in Table \ref{tab-arguments-nonprob} and the obligatory ones are \code{data}, while \code{selection}, \code{outcome}, or \code{target} must be specified depending on the chosen method.


\begin{table}[ht!]
\centering
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{p{3.5cm}p{10cm}}
\hline
Argument & Description \\
\hline
    \code{data} & \code{data.frame} with data from the non-probability sample \\
    \code{selection} & \code{formula} for the selection (propensity) equation\\
    \code{outcome} & \code{formula} for the outcome equation\\
    \code{target} & \code{formula} with target variables\\
    \code{svydesign} & Optional \code{svydesign2} object\\
    \code{pop\_totals}, 
    \code{pop\_means}, 
    \code{pop\_size} & Optional named \code{vector} with population totals or means of the covariates and population size \\
    \code{method\_selection} & Link function for the IPW approach (\code{"logit"}, \code{"probit"}, \code{"cloglog"})\\
    \code{method\_outcome} & Specification of the MI approach (one of \code{c("glm", "nn", "pmm")}) \\
    \code{family\_outcome} & The GLM family for the MI approach (one of \code{c("gaussian", "binomial", "poisson")}) \\
    \code{subset} & Optional \code{vector} specifying a subset of observations to be used in the fitting process\\
    \code{strata} & Optional \code{vector} specifying strata \\
    \code{weights} & Optional \code{vector} of prior weights to be used in the fitting process\\
    \code{na\_action} & \code{function} indicating what should happen when the data contain \code{NA}'s\\
    \code{control\_selection},
    \code{control\_outcome}, 
    \code{control\_inference} & Control parameters for selection and outcome model and the variance estimation via the \code{controlSel}, \code{controlOut} and \code{controlInf} functions respectively \\
    \code{start\_selection}, \code{start\_outcome}  & Optional \code{vector} with starting values for the parameters of the selection and the outcome equation\\
    \code{verbose} & Logical value indicating if verbose output should be printed\\
    \code{se} & Logical value indicating whether to calculate and return the standard error of the estimated mean\\
    \code{...} & Additional optional arguments\\
\hline
\end{tabular}
}
\caption{\code{nonprob} function arguments description}
\label{tab-arguments-nonprob}
\end{table}

The \code{nonprob} function is used specify inference methods through specifying \code{selection} and \code{outcome} arguments. If out of these two \code{selection} is specified than the IPW estimators are used, if only the \code{outcome} then the MI approach is used and if both are specified the DR approach is applied. The package allows to provide either reference population data (via the \code{pop\_totals}, or \code{pop\_means} and \code{pop\_size}) or a probability sample declared by the \code{svydesign} argument (\code{svydesign2} class of from the \pkg{survey} package). Selection of the speficic inference method is done through \code{method\_selection}, \code{method\_outcome}, \code{family\_outcome}, \code{control\_selection} and \code{control\_outcome} arguments. Specification of variance estimation method is done via the \code{control\_inference} argument.

In addition to using the survey package for design-based inference when probability samples are available, it also supports the various methods for estimating propensity scores and outcome models described in this thesis, such as logistic regression, complementary log-log models, probit models, generalized linear models, nearest neighbour algorithms and predictive mean matching.

Resulting object of class \code{nonprobsvy} is a list that contains the following (most important) elements:

\begin{itemize}
\item \code{data} -- an \code{data.frame} containing non-probability sample.
\item \code{X} -- a \code{matrix} containing both samples,
\item \code{y} -- a list containing all variables declared in either \code{target} or \code{outcome} arguments,
\item \code{R} -- a numeric \code{vector} informing about inclusion into non-probability sample,
\item \code{weights} -- propensity score weights or NULL (for the MI estimators),
\item \code{output} - a \code{data.frame} containing point and standard error estimates,
\item \code{outcome} -- a \code{list} of results for each \code{outcome} models,
\item \code{selection} -- a \code{list} of results for the \code{selection} model.
\item \code{svydesign} -- a \code{svydesign2} object passed by the \code{svydesign} argument.
\end{itemize}


After this neat description of the main functionality of the package, we will move on to some examples of its use. We will show how to define the given arguments in order to obtain estimates of interest as a result. We will be less interested in the results than in the way they are presented. There will be room in the following chapters for an analysis of simulations and applications of the package to the real world. We will focus on the three main estimators, as function calls for other functionalities such as variable selection, other linking functions or mass imputation methods.

## Controlling inference methods and the variance estimation

We provide three control function that allow users to specify the exact inference methods and the variance estimation.  The \code{controlSel} function provides essential control parameters for fitting the selection model in the \code{nonprob} function. It allows users to select between the MLE or GEE (calibrated) approach through \code{est_method_sel} (and the type with the \code{h} argument), specify the optimizer (\code{optimizer}) and the which variable selection should be applied (using different penalty functions like SCAD, lasso, and MCP through \code{penalty}) along with parameters (e.g. number of folds via the \code{nfolds} argument). The package uses the \pkg{nleqslv} package and fitting parameters (arguments starting with the \code{nleqslv*}).

The \code{controlOut} to fine-tune various aspects of the estimation process, including the variable selection methods (through different penalty options like SCAD, LASSO, and MCP with their respective tuning parameters), and detailed configuration for NN and PMM approaches (using parameters like \code{predictive_match}, \code{pmm_weights}, and \code{pmm_k_choice}). 

Finally, the \code{controlInf} function configures the parameters for variance estimation in the \code{nonprob} function. It allows user to specify whether the analytical or bootstrap approach should be used (the \code{var_method} argument), whether the variable selection should be applied (the \code{vars_selection} argument) and what type of boostrap should be applied for the probability sample (the \code{rep_type} argument). This function also allow to specify the how the inference for the DR approach should be taken: if a union or a division of variables after variable selection was applied (the \code{bias_inf} argument) and if the bias correction should be applied (the \code{bias_correction} argument).

## Design of the package

+ we do not provide ways to assess the models: this should be done prior the estimation stage and there are plenty of packages to do so
+ we provide some basic functionalities such as `hatvalues` or `cooks.distance` but in a limited form

In the next sections we present a case study on integration of non-probability sample with a reference probability sample. We will present various estimators and compare them. Finally, we present more advanced options of the package.

# Data analysis example {#sec-data-analysis}

## Description of the data

Before we explain the case study let's first load the package.

```{r load-package, message=FALSE, warning=FALSE}
library(nonprobsvy) ## for estimation
library(ggplot2) ## for visualisation
```

The goal of the case study to integrate administrative (\code{admin}) and survey (\code{jvs}) data about job vacancies in Poland. The first source is the Job Vacancy Survey (JVS) with a sample of 6,523 units. The survey is based on a probability sample drawn according to proportional-to-size stratified sampling design. The details regarding the survey can be found in \cite{jvs2022}. The dataset contains about The Nomenclature of Economic Activities (NACE; 14 levels, \code{nace} column), \code{region} (16 levels), sector (2 levels, \code{private} column), size of the entity (3 levels: Small, Medium and Large) and the final weight (i.e. design weight corrected for non-contact and non-response).


```{r, out.width="80%"}
data(jvs)
head(jvs)
```

As the package leverage the \pkg{survey} package functionalities we need to define the \code{svydesign2} object via the \code{svydesign} function as presented below. The dataset does not contain the true stratification variable we use a simplified version by specifying `~ size + nace + region` and we do not know have information on the non-response and its correction we simply assume that the `weight` is the calibrated weight that sums up to the population size.

```{r}
jvs_svy <- svydesign(ids = ~ 1, 
                     weights = ~ weight,
                     strata = ~ size + nace + region,
                     data = jvs)
```

The second source is the Central Job Offers Database (CBOP), which is a register of all vacancies submitted to Public Employment Offices (see \url{https://oferty.praca.gov.pl}). We treat this as the \textit{non-probability sample} because is voluntary administrative data and inclusion mechanism is unknown. This dataset was prepared in such way that the records out of scope (either by the definition of vacancy or population of entities) were excluded. The dataset contains the same variables as JVS with one additional \code{single\_shift} which is our target variable defined as: \texttt{whether a company seeks at least one employee for a single-shift job}. The goal of this case study is to estimate \textit{the share of companies that seeks employees for a single-shift job} in Poland in a given quarter. 

```{r}
data(admin)
head(admin)
```

Please note that, this paper does not aim to provide full tutorial on using non-probability samples for statistical inference. Thus, we skipped the part of aligning variables to meet the same definitions, assessing how strong is the relation between auxiliary variables, target variable and selection mechanism and distribution mis-matches between both samples. In the examples below we assume that there is no overlap between two sources and the nai"ve, reference estimate, given by a simple mean of the `single_shift` column of `admin` equals to `r round(mean(admin$single_shift)*100,1)`%.

## Estimation

### Propensity score approach

First, we start with the IPW approach with to possible estimation methods MLE (standard) and GEE (calibrated to the estimated survey totals). We start by calling the \code{nonprob} function where we define the \code{selection} argument responsible for the formulae for the inclusion variables, the \code{target} argument which specifies the variable of interest \code{single_shift}. The rest refer to the \code{svydesign} object, dataset and specification of the link function (\code{method_selection}).

```{r}
ipw_est1 <- nonprob(
  selection = ~ region + private + nace + size,
  target = ~ single_shift,
  svydesign = jvs_svy,
  data = admin,
  method_selection = "logit" ## this is the default
)
```

In order to get the basic information about the estimated target quantity we can use the \code{print} method the object. It provides the call and the estimated mean, standard error (\code{SE}) and 95\% confidence interval (\code{lower_bound} and \code{upper_bound}).

```{r}
ipw_est1
```

If we are interested in a detailed information about the model we can use the \code{summary} method. 

```{r}
summary(ipw_est1)
```

This displays information on the datasets used for estimation (probability and non-probability sample). The estimated regression coefficients are also shown, in this case for the logit model for propensity score model (section \code{Regression coefficients}). In addition, for diagnostic purposes, we have access to the distribution of the weights calculated from the inclusion probabilities (section \code{Weights}), the distribution of the residuals from the model (section \code{Residuals}), as well as the values of the AIC, BIC statistics in the case of models based on MLE. 

If we are interested in the calibrated IPW, one needs to define a `controlSel` function in the `control_selection` argument with the `est_method_sel` argument equal to `gee` (the default is `mle`) and set the value of `h` (as dicussed in the \ref{sec-ipw-gee} section).

```{r}
ipw_est2 <- nonprob(
  selection = ~ region + private + nace + size,
  target = ~ single_shift,
  svydesign = jvs_svy,
  data = admin,
  method_selection = "logit",
  control_selection = controlSel(h = 1, est_method_sel = "gee")
)
```

Results are comparable to the standard IPW point estimate (`r round(ipw_est2$output$mean*100,1)` vs `r round(ipw_est1$output$mean*100,1)`) while the standard error is lower higher. 

```{r}
ipw_est2
```

The calibrated IPW significantly improves the balance as can be accessed by the `nonprobsvycheck` function:

```{r}
data.frame(ipw_mle=nonprobsvycheck(~size, ipw_est1, 1)$balance,
           ipw_gee=nonprobsvycheck(~size, ipw_est2, 1)$balance)
```

Notice that, neither in the package nor this paper we focus a detailed description of the post-hoc results, such as covariate balance. This can be done via existing CRAN packages, for instance using the \code{bal.tab} function from the \pkg{cobalt} package \citep{cobalt}.

### Prediction-based approach

If a user is interested in a prediction-based approach, in particular mass imputation estimators, then should specify the argument `outcome` as a formulae (similarly as in the `glm` function). We allow single outcome (specified as `y ~ x1 + x2 + ... + xk`) or multiple outcomes (as `y1 + y2 + y2 ~ x1 + x2 + ... + xk`). Note that if the `outcome` argument is specified then there is no need to specify `target` argument. By default GLM type of the MI estimator is assumed (i.e. `method_outcome="glm"`). In the code below we present possible way to declare this type of the MI estimator.

```{r}
mi_est1 <- nonprob(
  outcome = single_shift ~ region + private + nace + size,
  svydesign = jvs_svy,
  data = admin,
  method_outcome = "glm",
  family_outcome = "binomial"
)

mi_est1
```


If a user is interested in the nearest neighbours MI estimator one can specify `method_outcome = "nn"` for the nearest neighbours search using all variables specified in the `outcome` argument, or `method_outcome = "pmm"` if is interested in predictive mean matching. In both cases we are using $k=5$ nearest neighbours (i.e. `controlOut(k=5)`).  For the NN MI estimator there is no need to specify the `family_outcome` argument as no model is estimated underneath. For both approaches we use the \pkg{RANN} package \citep{rann-pkg}. 

```{r}
mi_est2 <- nonprob(
  outcome = single_shift ~ region + private + nace + size,
  svydesign = jvs_svy,
  data = admin,
  method_outcome = "nn",
  control_outcome = controlOut(k=5)
)

mi_est3 <- nonprob(
  outcome = single_shift ~ region + private + nace + size,
  svydesign = jvs_svy,
  data = admin,
  method_outcome = "pmm",
  family_outcome = "binomial", 
  control_outcome = controlOut(k=5)
)
```

Results of both estimators seems to be similar, but it should be noted that the NN MI estimator suffers from the curse of dimensionality so one should trust more the PMM MI estimator.

```{r}
mi_est2$output
mi_est3$output
```

As discussed in Section \ref{sec-methods} both IPW and MI estimators are asymptotically unbiased only when the model and auxiliary variables are correctly specified. To overcome this problem we focus now on the doubly robust estimators.

### The doubly robust approach

To indicate that the doubly robust estimation should used user needs to specify both the `selection` and `outcome` arguments. These formulas can be specified with the same or varying number of aixuliary variables. We also allow, simiarly as in the MI approach, multiple outcomes. In the following example code we specified the non-calibrated IPW and the GLM MI estimator. 

```{r}
dr_est1 <- nonprob(
  selection = ~ region + private + nace + size,
  outcome = single_shift ~ region + private + nace + size,
  svydesign = jvs_svy,
  data = admin,
  method_selection = "logit",
  method_outcome = "glm",
  family_outcome = "binomial"
)
dr_est1
```

Detailed results can be obtained by using `summary` function which prints both set of coefficients for the outcome and selection models. We omit this output due to limited space of the paper. Finally, we can use bias minimisation approach as proposed by \citet{yang_doubly_2020} by specifying `control_inference = controlInf(bias_correction = TRUE)` argument. This part is implemented in the \pkg{Rcpp} \citep{Rcpp} and \pkg{RcppArmadillo} \citep{RcppArmadillo} packages for performance. 

```{r}
dr_est2 <- nonprob(
  selection = ~ region + private + nace + size,
  outcome = single_shift ~ region + private + nace + size,
  svydesign = jvs_svy,
  data = admin,
  method_selection = "logit",
  method_outcome = "glm",
  family_outcome = "binomial",
  control_inference = controlInf(bias_correction = TRUE)
)
dr_est2
```


## Comparison of estimates

Finally, as there is no single method for non-probability samples we suggest to compare results in a single table or a plot. In the Figure ... we presents point estimates along with 95\% confidence intervals. The various estimators show interesting patterns compared to the naive estimate (red dashed line). MI estimators demonstrate notably different behaviours: while PMM produces the highest point estimate with the widest confidence interval, NN yields the lowest estimate, close to the naive value. The other estimators - MI (GLM), IPW (both MLE and GEE), and DR (with and without bias minimization) -- cluster together with similar point estimates and confidence interval widths, suggesting some consensus in their bias correction. These methods all indicate a population parameter higher than the naive estimate, but their relative consistency, except for the extreme estimates from MI (PMM) and MI (NN), provides some confidence in their bias correction capabilities.

```{r comparison-of-est, fig.cap="Comparison of estimates of the share of job vacancies offered on a single-shift", fig.align='center', fig.pos='ht!'}
dr_summary <- rbind(cbind(ipw_est1$output, ipw_est1$confidence_interval),
                    cbind(ipw_est2$output, ipw_est2$confidence_interval),
                    cbind(mi_est1$output, mi_est1$confidence_interval),
                    cbind(mi_est2$output, mi_est2$confidence_interval),
                    cbind(mi_est3$output, mi_est3$confidence_interval),
                    cbind(dr_est1$output, dr_est1$confidence_interval),
                    cbind(dr_est2$output, dr_est2$confidence_interval))
rownames(dr_summary) <- NULL
dr_summary$est <- c("IPW (MLE)", "IPW (GEE)", "MI (GLM)", "MI (NN)", 
                    "MI (PMM)", "DR", "DR (BM)")
ggplot(data = dr_summary, 
       aes(y = est, x = mean, xmin = lower_bound, xmax = upper_bound)) + 
  geom_point() + 
  geom_vline(xintercept = mean(admin$single_shift), 
             linetype = "dotted", color = "red") + 
  geom_errorbar() + 
  labs(x = "Point estimator and confidence interval", y = "Estimators")

```


## Advanced usage

### Bootstrap Approach for Variance Estimation

In the package we allow user to estimate variance of the mean using analytical (default) or bootstrap approach. In case of analytical variance estimators we use the estimators proposed in the papers described in the Section \ref{sec-methods}. Users may disable standard error calculation using \code{nonprob(se=FALSE)}. The bootstrap approach implemented in the package refers to two samples: 

\begin{itemize}
\item non-probability -- we currently support only simple random sampling with replacement,
\item probability -- we support all the approaches implemented in the \code{as.svrepdesign} and we refere the reader to the help file of this function. Currently we do not support the 
\end{itemize}

The bootstrap approach is done in the following way: 1) we independently draw the same number of $B$ bootstrap samples from non-probability and probability survey; 2) we estimate population mean based on selected method (e.g. the DR approach); and 3) calculate bootstrap standard error using the following formulae

\begin{equation}
aaa
\end{equation}

To specify the bootstrap approach one should use \code{controlInf()} function with `var_method = "bootstrap"`. Controlling the bootstrap method for probability sample is done by `rep_type` argument which passes the method to the `as.svrepdesign` function. The number of iterations is set in the `num_boot` argument (default 100). If the samples are large or the estimation method is complicated (e.g. involves variable selection) one can set `verbose=TRUE` to track the progress. By default results of bootstrap are stored in the `boot_sample` element of the resulting list (to disable this `keep_boot` should be set to `FALSE`). The following code provides an example of using the IPW approach with the bootstrap approach specified by the argument `control_inference` of the `nonprob` function. 


```{r}
ipw_est1_boot <- nonprob(
  selection = ~ region + private + nace + size,
  target = ~ single_shift,
  svydesign = jvs_svy,
  data = admin,
  method_selection = "logit",
  control_inference = controlInf(var_method = "bootstrap", num_boot = 50),
  verbose = F
)
```

Next, we compare the estimated standard error with the analytical one below.

```{r}
rbind(ipw_est1$output,
      ipw_est1_boot$output)
```
To assess the samples one can access the `boot_sample` element of the output list of the `nonprob` function. Note that this is returned as `matrix` because we allow multiple `target` variables.

```{r}
head(ipw_est1_boot$boot_sample, n=3)
```

### Variable Selection Algorithms

In this section we briefly present how to use variable selection algorithms. In order to specify that a variable selection algorithm should be used one should specify the `control_inference = controlInf(vars_selection = TRUE)` argument. Then, the user should either leave the default or specify the parameters for the outcome via the \code{controlOut} function or selection outcome (\code{controlSel}). Both function have the same parameters: 

\begin{itemize}
\item \code{penalty} -- The penanlization function used during variables selection (possible values: \code{c("SCAD", "lasso", "MCP")})
\item \code{nlambda} -- The number of $\lambda$ values. Default is 50.
\item \code{lambda_min} -- The smallest value for $\lambda$, as a fraction of \code{lambda.max}. Default is .001.
\item \code{lambda} -- A user specified vector of lambdas (only for the \code{controlSel} function).
\item \code{nfolds} -- The number of folds for cross validation. Default is 10.
\item \code{a_SCAD, a_MCP} -- The tuning parameter of the SCAD and MCP penalty for selection model. Default is 3.7 and 3 respectively.
\end{itemize}

For the MI approach we leverage the \pkg{ncvreg} package \citep{ncvreg} as it is solely package that uses the SCAD method in R. While for the IPW and DR approaches we developed our own codes in \proglang{C++} via the \pkg{Rcpp} package. In the code below we apply variable selection for the MI GLM estimator using only 5 folds, 25 possible values of $\lambda$ parameters and apply the LASSO penalty.

```{r}
mi_est1_sel <- nonprob(
  outcome = single_shift ~ region + private + nace + size,
  svydesign = jvs_svy,
  data = admin,
  method_outcome = "glm",
  family_outcome = "binomial" ,
  control_outcome = controlOut(nfolds = 5, nlambda = 25, penalty = "lasso"),
  control_inference = controlInf(vars_selection = TRUE),
  verbose = TRUE
)
```

In this case study the MI GLM estimator with variable selection yields almost the same results as the approach without it. Point estimates and standard errors differ at the fourth and third digit respectively.

```{r}
rbind("MI without var sel"=mi_est1$output,
      "MI with var sel"=mi_est1_sel$output)
```

# Classes and S3methods {#sec-s3methods}

<!--- TODO
nobs
pop.size
residuals
cooks.distance
hatvalues
loglik
AIC
BIC
confint
vcov
deviance
-->

# Summary and future work

The \pkg{nonprobsvy} package provides a comprehensive R toolkit for addressing inference challenges with non-probability samples by integrating them with probability samples or known population totals/means. As non-probability data sources like administrative data, voluntary online panels, and social media data become increasingly available, statisticians need robust methods to produce reliable population estimates. The package implements \textit{state-of-the-art} approaches including mass imputation, inverse probability weighting (IPW), and doubly robust (DR) methods, each designed to correct selection bias by leveraging auxiliary data. By providing a unified framework and integration with the \pkg{survey} package, \pkg{nonprobsvy} makes complex statistical methods for non-probability samples more accessible, enabling researchers to produce robust estimates even when working with non-representative data. 

There are several avenues for future development of the \pkg{nonprobsvy} package. A key priority is implementing model-based calibration and additional methods for estimating propensity scores and weights. The package currently assumes no overlap between probability and non-probability samples, so accounting for potential overlap (e.g., in big data sources and registers) is another important extension. Additional planned developments include handling non-ignorable sample selection through sample selection models, maintaining consistency with calibration weights, and supporting multiple non-probability samples for data integration from various sources.

Further methodological extensions under consideration include empirical likelihood approaches for doubly/multiply robust estimation, integration of machine learning methods like debiased/double machine learning from causal inference, handling measurement error in big data variables, and expanding the bootstrap approach beyond simple random sampling with replacement. The package will also be extended to work with the `svyrep.design` class from the \pkg{survey} package and the \pkg{svrep} package. These developments will enhance \pkg{nonprobsvy}'s capabilities for handling complex survey data structures and modern estimation challenges.

# Acknowledgements {#sec-acknowledgements}

The authors' work has been financed by the National Science Centre in Poland, OPUS 20, grant no. 2020/39/B/HS4/00941.

Łukasz Chrostowski is the main developer and maintainer of the package. He was also responsible for the first draft of the paper as it is based on his Master's thesis (available at \url{https://github.com/ncn-foreigners/graduation-theses}).
Piotr Chlebicki contributed to the package and implemented PMM estimators. Maciej Beręsewicz was responsible for the initial idea and the design of the package, testing, reviewing and small contributions code and prepared the final manuscript.

We would like to thank ...

\clearpage

\appendix

# List of symbols

\begin{table}[ht!]
\centering
\begin{tabular}{ll}
\hline
\textbf{Symbol} & \textbf{Description} \\
\hline
$U$ & Target population of size $N$ \\
$N$ & Population size \\
$\bx_i$ & Vector of auxiliary variables for unit $i$ \\
$y_i$ & Study variable for unit $i$ \\
$S_A$ & Non-probability sample \\
$S_B$ & Probability sample \\
$n_A$ & Size of non-probability sample \\
$n_B$ & Size of probability sample \\
$\pi_i$ & Inclusion probability for unit $i$ in probability sample \\
$d_i$ & Design weight ($1/\pi_i$) for unit $i$ \\
$R_i$ & Indicator of inclusion into non-probability sample \\
$\mu_y$ & Population mean of target variable $Y$ \\
$\pi_i^A$ & Propensity score for unit $i$ in non-probability sample \\
$m(\bx_i, \bbeta)$ & Regression model for outcome variable \\
$\hat{\mu}_{IPW}$ & Inverse probability weighting estimator \\
$\hat{\mu}_{MI}$ & Mass imputation estimator \\
$\hat{\mu}_{DR}$ & Doubly robust estimator \\
$\btheta$ & Parameter vector for selection model \\
$\bbeta$ & Parameter vector for outcome model \\
$\hat{y}_i$ & Imputed value for unit $i$ \\
$\hat{N}^A$ & Estimated size based on non-probability sample \\
$\hat{N}^B$ & Estimated size based on probability sample \\
\hline
\end{tabular}
\caption{List of symbols and their descriptions}
\label{tab-list-of-sybmols}
\end{table}

\clearpage


# Algorithms {#sec-details}

\begin{algorithm}[ht!]
\caption{Mass imputation based on a generalized linear model}
\label{algo-1}
\begin{algorithmic}[1]
 \State Estimate the regression model $\mathbb{E}[Y|\boldsymbol{X}=\boldsymbol{x}]=m(\boldsymbol{x}, \boldsymbol{\beta})$\ basing on units from $S_A$ sample.
 \State For each $i \in S_B$, calculate the imputed value as
 $$
 \hat{y}_i = m\left(\boldsymbol{x}_{i},\hat{\boldsymbol{\beta}}\right).
 $$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[ht!]
\caption{Mass imputation using the k-nearest-neighbour algorithm}
\label{algo-2}
\begin{algorithmic}[1]
\State If $k=1$, then for each $i \in S_B$ match $\hat{\nu}(i)$ such that
$\displaystyle \hat{\nu}(i)=
\argmin_{j\in S_{A}}d\left(\bx_i,\bx_j\right)$.
\State If $k>1$, then
$$\hat{\nu}(i, z) = \argmin_{\displaystyle j\in S_{A}\setminus\bigcup_{t=1}^{z-1}
\{\hat{\nu}(i, t)\}} d\left(\bx_i, \bx_j\right)$$
i.e. $\hat{\nu}(i, z)$ is $z$-th nearest neighbour from the sample.\;
\State For each $i \in S_B$, calculate the imputed value as
$$
\hat{y}_i = \frac{1}{k}\sum_{t=1}^{k}y_{\hat{\nu}(i, t)}.
$$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht!]
\caption{$\hat{y}-\hat{y}$ Imputation:}
\label{algo-3}
\begin{algorithmic}[1]
\State Estimate regression model $\mathbb{E}[Y|\boldsymbol{X}=\boldsymbol{x}]=m(\boldsymbol{x}, \boldsymbol{\beta})$.\;
\State Impute $$\hat{y}_{i}=m\left(\boldsymbol{x}_{i},\hat{\boldsymbol{\beta}}\right), 
\hat{y}_{j}=m\left(\boldsymbol{x}_{j},\hat{\boldsymbol{\beta}}\right)$$
for $i\in S_{B}, j\in S_{A}$ and assign each 
$i\in S_{B}$ to $\hat{\nu}(i)$, where
$$\displaystyle \hat{\nu}(i)=
\argmin_{j\in S_{A}}\lVert \hat{y}_{i}-\hat{y}_{j}\rVert$$ or
$$\displaystyle \hat{\nu}(i)=
\argmin_{j\in S_{A}}d\left(\hat{y}_{i},\hat{y}_{j}\right)$$ if $d$ is not induced by the norm.\;

\State If $k>1$, then:
$$\hat{\nu}(i, z) = \argmin_{\displaystyle j\in S_{A}\setminus\bigcup_{t=1}^{z-1}
\{\hat{\nu}(i, t)\}} d\left(\hat{y}_{i},\hat{y}_{j}\right)$$
e.g., $\hat{\nu}(i, z)$ is $z$-th nearest neighbor from a sample.\;
\State For $i \in S_B$, calculate imputation value as 
$$
\hat{y}_i = \frac{1}{k}\sum_{t=1}^{k}y_{\hat{\nu}(i, t)}.
$$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht!]
\caption{$\hat{y}-y$ Imputation:}
\label{algo-4}
\begin{algorithmic}[1]
\State Estimate regression $\mathbb{E}[Y|\boldsymbol{X}=\boldsymbol{x}]=m(\boldsymbol{x}, \boldsymbol{\beta})$.\;
\State Impute $\hat{y}_{i}=m\left(\boldsymbol{x}_{i},\hat{\boldsymbol{\beta}}\right)$ 
for $i \in S_{B}$ and assign each 
$i \in S_{B}$ do $\hat{\nu}(i)$, where
$\displaystyle \hat{\nu}(i)=
\argmin_{j \in S_{A}}\lVert \hat{y}_{i}-y_{j}\rVert$ or
$\displaystyle \hat{\nu}(i)=
\argmin_{j \in S_{A}}d\left(\hat{y}_{i},y_{j}\right)$ 
if $d$ not induced by the norm.\;
\State If $k>1$, then:
$$\hat{\nu}(i, z) = \argmin_{\displaystyle j \in S_{A} \setminus \bigcup_{t=1}^{z-1}
\{\hat{\nu}(i, t)\}}
d\left(\hat{y}_{i},y_{j}\right).$$
\State For each $i \in S_B$ calculate imputation value as
$$
\hat{y}_i = \frac{1}{k}\sum_{t=1}^{k}y_{\hat{\nu}(i, t)}.
$$
\end{algorithmic}
\end{algorithm}


\clearpage


# Codes for specific methods {#sec-examples}

In this section we provide list of methods along with the appropriate set up of the `nonprob` function. We divide this section into two groups: 1) unit-level data from the reference probability sample is available; 2) only a vector of population totals or means (with population size) is available.

## Unit-level probability sample is available

1. Mass imputation based on regression imputation


```{r, eval=FALSE}
nonprob(
  outcome = y ~ x1 + x2, 
  data = nonprob, 
  svydesign = prob, 
  method_outcome = "glm", 
  family_outcome = "gaussian"
)
```

2. Mass imputation based on nearest neighbour imputation

```{r, eval=FALSE}
nonprob(
  outcome = y ~ x1 + x2, 
  data = nonprob, 
  svydesign = prob, 
  method_outcome = "nn", 
  family_outcome = "gaussian", 
  control_outcome = controlOutcome(k = 2)
)
```


3. Mass imputation based on predictive mean matching

```{r, eval=FALSE}
nonprob(
  outcome = y ~ x1 + x2, 
  data = nonprob, 
  svydesign = prob, 
  method_outcome = "pmm", 
  family_outcome = "gaussian"
)
```


4. Mass imputation based on regression imputation with variable selection (LASSO)

```{r, eval=FALSE}
nonprob(
  outcome = y ~ x1 + x2, 
  data = nonprob, 
  svydesign = prob, 
  method_outcome = "pmm", 
  family_outcome = "gaussian", 
  control_outcome = controlOut(penalty = "lasso"), 
  control_inference = controlInf(vars_selection = TRUE)
)
```

5. Inverse probability weighting (MLE)

```{r, eval=FALSE}
nonprob(
  selection =  ~ x1 + x2, 
  target = ~ y, 
  data = nonprob, 
  svydesign = prob, 
  method_selection = "logit"
)
```


6. Inverse probability weighting with calibration constraint (GEE)

```{r, eval=FALSE}
nonprob(
  selection =  ~ x1 + x2, 
  target = ~ y, 
  data = nonprob, 
  svydesign = prob, 
  method_selection = "logit", 
  control_selection = controlSel(est_method_sel = "gee", h = 1)
)
```


7. Inverse probability weighting with calibration constraint (GEE) with variable selection (SCAD)

```{r, eval=FALSE}
nonprob(
  selection =  ~ x1 + x2, 
  target = ~ y, 
  data = nonprob, 
  svydesign = prob, 
  method_outcome = "pmm", 
  family_outcome = "gaussian", 
  control_inference = controlInf(vars_selection = TRUE)
)
```


8. Doubly robust estimator

```{r, eval=FALSE}
nonprob(
  selection = ~ x1 + x2, 
  outcome = y ~ x1 + x2, 
  data = nonprob, 
  svydesign = prob, 
  method_outcome = "glm", 
  family_outcome = "gaussian"
)
```


9. Doubly robust estimator with variable selection (SCAD) and bias minimization

```{r, eval=FALSE}
nonprob(
  selection = ~ x1 + x2, 
  outcome = y ~ x1 + x2, 
  data = nonprob, 
  svydesign = prob,
  method_outcome = "glm", 
  family_outcome = "gaussian", 
  control_inference = controlInf(
    vars_selection = TRUE, 
    bias_correction = TRUE
  )
)
```


## Only population totals or means are available

Example declarations

1. Mass imputation based on regression imputation

```{r, eval=FALSE}
nonprob(
  outcome = y ~ x1 + x2,
  data = nonprob,
  pop_totals = c(`(Intercept)`= N,
                 x1 = tau_x1,
                 x2 = tau_x2),
  method_outcome = "glm",
  family_outcome = "gaussian"
)
```

2. Inverse probability weighting

```{r, eval=F}
nonprob(
  selection =  ~ x1 + x2, 
  target = ~ y, 
  data = nonprob, 
  pop_totals = c(`(Intercept)` = N, 
                 x1 = tau_x1, 
                 x2 = tau_x2), 
  method_selection = "logit"
)
```

3. Inverse probability weighting with calibration constraint

```{r, eval=F}
nonprob(
  selection =  ~ x1 + x2, 
  target = ~ y, 
  data = nonprob, 
  pop_totals = c(`(Intercept)`= N, 
                 x1 = tau_x1, 
                 x2 = tau_x2), 
  method_selection = "logit", 
  control_selection = controlSel(est_method_sel = "gee", h = 1)
)
```

4. Doubly robust estimator

```{r, eval=F}
nonprob(
  selection = ~ x1 + x2, 
  outcome = y ~ x1 + x2, 
  pop_totals = c(`(Intercept)` = N, 
                 x1 = tau_x1, 
                 x2 = tau_x2), 
  method_outcome = "glm", 
  family_outcome = "gaussian"
)
```


\clearpage



# Detailed derivations {#sec-derivations}

\begin{table}[ht!]
\small
\caption{MLE Functions and Gradients for Different Link Functions}
\centering
\begin{tabular}{p{2cm} p{6.5cm} p{6.5cm}}
\hline
Link & MLE Function & Gradient \\ 
\hline
\code{logit} & 
$\displaystyle
\begin{aligned}
& \sum_{i \in S_{\mathrm{A}}} \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \\
& {} - \sum_{i \in S_{\mathrm{B}}} d_i^{\mathrm{B}} \log \left[ 1 + \exp \left( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \right) \right]
\end{aligned}
$ & 
$\displaystyle
\begin{aligned}
& \sum_{i \in S_{A}} \boldsymbol{x}_{i} \\
& {} - \sum_{i \in S_{B}} d_{i}^{B} \pi(\boldsymbol{x}_{i}, \boldsymbol{\theta}) \boldsymbol{x}_{i}
\end{aligned}
$ \\ \hline

\code{probit} & 
$\displaystyle
\begin{aligned}
& \sum_{i \in S_{A}} \log \left( \frac{ \Phi( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} ) }{ 1 - \Phi( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} ) } \right) \\
& {} + \sum_{i \in S_{B}} d_{i}^{B} \log \left[ 1 - \Phi( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} ) \right]
\end{aligned}
$ & 
$\displaystyle
\begin{aligned}
& \sum_{i \in S_A} \frac{ \phi( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} ) }{ \Phi( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} ) [ 1 - \Phi( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} ) ] } \boldsymbol{x}_i \\
& {} - \sum_{i \in S_B} d_i^B \frac{ \phi( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} ) }{ 1 - \Phi( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} ) } \boldsymbol{x}_i
\end{aligned}
$ \\ \hline

\code{cloglog} & 
$\displaystyle
\begin{aligned}
& \sum_{i \in S_{A}} \left\{ \log \left[ 1 - \exp \left( -\exp( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} ) \right) \right] \right\} \\
& {} + \exp( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} ) - \sum_{i \in S_{B}} d_{i}^{B} \exp( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} )
\end{aligned}
$ & 
$\displaystyle
\begin{aligned}
& \sum_{i \in S_{A}} \frac{ \exp( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} ) \boldsymbol{x}_{i} }{ \pi( \boldsymbol{x}_{i}, \boldsymbol{\theta} ) } \\
& {} - \sum_{i \in S_{B}} d_{i}^{B} \exp( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} ) \boldsymbol{x}_{i}
\end{aligned}
$ \\ \hline

\end{tabular}
\end{table}
