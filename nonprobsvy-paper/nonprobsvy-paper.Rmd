---
documentclass: jss
author:
  - name: Łukasz Chrostowski
    affiliation: Adam Mickiewicz University
    affiliation2: Pearson
    # use this syntax to add text on several lines
    address: |
      | First line
      | Second line
    email: \email{lukchr@st.amu.edu.pl}
    url: https://posit.co
  - name: Piotr Chlebicki
    orcid: 0009-0006-4867-7434
    address: |
      | Department of Statistics and Mathematics,
      | Faculty of Biosciences,
      | Universitat Autònoma de Barcelona
    affiliation: |
      | Stockholm University \AND
    email: \email{piotr.chlebicki@math.su.se}
  - name: Maciej Beręsewicz
    orcid: 0000-0002-8281-4301
    address: |
      | Department of Statistics,
      | Institute of Informatics and Electronic Economy,
      | Poznań University of Economics and Business
    affiliation: |
      | Poznań University of Economics and Business
    affiliation2: 'Statistical Office in Poznań'
    # To add another line, use \AND at the end of the previous one as above
    email: \email{maciej.beresewicz@ue.poznan.pl}
    url: https://posit.co
title:
  formatted: "\\pkg{nonprobsvy} -- An R package for modern methods for non-probability surveys"
  # If you use tex in the formatted title, also supply version without
  plain:     "nonprobsvy -- An R package for modern methods for non-probability surveys"
  # For running headers, if needed
  short:     "\\pkg{nonprobsvy} for non-probability surveys"
abstract: >
  The abstract of the article.
keywords:
  # at least one keyword must be supplied
  formatted: [data integration, doubly robust estimation, propensity score estimation, mass imputation, "\\proglang{R}"]
  plain: [data integration, doubly robust estimation, propensity score estimation, mass imputation, R]
preamble: >
  \usepackage{amsmath}
  \usepackage{amssymb}
  \usepackage[T1]{fontenc}
  \usepackage{lmodern}
  \usepackage[english]{babel}
  \selectlanguage{english}
  \usepackage{physics}
  \usepackage{amsfonts}
  \usepackage{graphicx}
  \usepackage{enumitem}
  \usepackage{float}
  \usepackage{tabularx}
  \usepackage{amsthm}
  \usepackage{geometry}
  \usepackage{xcolor}
  \pagestyle{headings}
  \usepackage{fancyhdr} 
  \usepackage{color} 
  \usepackage{multirow}
  \usepackage{booktabs}
  \usepackage{url}
  \usepackage{array}
  \usepackage{graphicx}
  \usepackage[mathcal]{eucal}
  \usepackage{algorithm}
  \usepackage{algpseudocode}
  \usepackage{listings}
  \usepackage{makecell}
  \usepackage{inconsolata}
  \usepackage{lscape}
  \usepackage{pdflscape}
  \usepackage{makecell}
  \usepackage{enumitem}
  \usepackage{siunitx}
  \usepackage{multirow}
  \usepackage{caption}
  \usepackage{booktabs}
  \newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
  \newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
  \newcommand{\bX}{\boldsymbol{X}}
  \newcommand{\bx}{\boldsymbol{x}}
  \newcommand{\bY}{\boldsymbol{Y}}
  \newcommand{\by}{\boldsymbol{y}}
  \newcommand{\bh}{\boldsymbol{h}}
  \newcommand{\bH}{\boldsymbol{H}}
  \newcommand{\ba}{\boldsymbol{a}}
  \newcommand{\bp}{\boldsymbol{p}}
  \newcommand{\bA}{\boldsymbol{A}}
  \newcommand{\bw}{\boldsymbol{w}}
  \newcommand{\bd}{\boldsymbol{d}}
  \newcommand{\bZ}{\boldsymbol{Z}}
  \newcommand{\bz}{\boldsymbol{z}}
  \newcommand{\bv}{\boldsymbol{v}}
  \newcommand{\bu}{\boldsymbol{u}}
  \newcommand{\bU}{\boldsymbol{U}}
  \newcommand{\bQ}{\boldsymbol{Q}}
  \newcommand{\bG}{\boldsymbol{G}}
  \newcommand{\HT}{\text{\rm HT}}
  \newcommand{\bbeta}{\boldsymbol{\beta}}
  \newcommand{\balpha}{\boldsymbol{\alpha}}
  \newcommand{\btau}{\boldsymbol{\tau}}
  \newcommand{\bgamma}{\boldsymbol{\gamma}}
  \newcommand{\btheta}{\boldsymbol{\theta}}
  \newcommand{\blambda}{\boldsymbol{\lambda}}
  \newcommand{\bPhi}{\boldsymbol{\Phi}}
  \newcommand{\bEta}{\boldsymbol{\eta}}
  \newcommand{\bZero}{\boldsymbol{0}}
  \newcommand{\colvec}{\operatorname{colvec}}
  \newcommand{\logit}{\operatorname{logit}}
  \newcommand{\Exp}{\operatorname{Exp}}
  \newcommand{\Ber}{\operatorname{Bernoulli}}
  \newcommand{\Uni}{\operatorname{Uniform}}
output: rticles::jss_article
bibliography: references.bib
---

```{r, setup, include=FALSE}
options(prompt = 'R> ', continue = '+ ')
```

# Introduction
With the availability of large sets of administrative data, voluntary internet panels, social media and big data, inference with non-probability samples is being heavily studied in the statistical literature @beaumont2020probability, @elliott_inference_2017, @berkesewicz2017two, @citro2014multiple. Because of their non-statistical character and unknown sampling mechanism, these sources cannot be used directly for estimating population characteristics. 

Several inference approaches have been proposed in the literature with respect to data from non-probability samples, which either involve data integration with population level data or probability samples from the same population (for recent review see @wu2022statistical).

Although probability samples are still the most popular standard among statisticians, the cost of obtaining them, in terms of time or capital, motivates the use of non-probability samples, which have to overcome other challenges. The first is that such samples generally do not represent the whole population, as can be said of probability samples. Another problem is the lack, or rather the ignorance, of the mechanism for selecting individuals for this type of sample, which does not allow the substantial use of existing statistical methods. For this reason, many different techniques have been proposed in the literature for integrating data in order to infer from available sources of different structural character. 

It should be noted that there are several packages that allow the correction of selection bias in nonprobability samples, such as:

1. @GJRM supports generalized joint regression modeling, enabling users to fit complex joint models for multivariate outcomes often needed in handling selection bias in nonprobability samples.
2. @NonProbEst provides methods specifically designed for estimating population parameters from nonprobability samples, employing approaches like propensity score adjustment and calibration weighting to correct for sample selection bias.
3. @sampling offers tools for implementing various sampling designs, including stratified, cluster, and unequal probability sampling, which are essential for creating representative samples and reducing selection bias in survey data analysis.

However, these packages do not implement state-of-the-art approaches recently proposed in the literature: @chen2020doubly, @yang_doubly_2020, @wu2022statistical nor do they use the survey package @lumley2004 for inference.

This paper describes the nonprobsvy package for inference with non--probability samples, available from the Comprehensive R Archive Network (CRAN) at [CRAN.R-project](https://CRAN.R-project.org/package=nonprobsvy). Development version of the package can be also found at [github](https://github.com/ncn-foreigners/nonprobsvy). 

Table \ref{tab:tabela1} shows the basic characteristics of each of the samples described. In particular, what are the advantages and disadvantages of each type of sample with respect to population coverage, bias, variance, costs, and the selection mechanism for observations into the samples.

\begin{table}
    \centering
    \caption{Probability and non-probability samples}
    \begin{tabular}{lll}
    \hline
    \textbf{Factor}     &  \textbf{Probability sample} & \textbf{Non-probability sample}\\
    \hline
    Selection     &  Sampling design & Auto-selection \\
    Coverage     &  Typically good & Certain groups are excluded \\
    Bias & Typically smaller & Large or very large \\
    Variance & Typically larger & Small, or very small \\
    Cost & Large or very large & Typically small \\
    \hline
    \end{tabular}
    \label{tab:tabela1}
\end{table}


# Methods for non-probability samples

## Basic setup

Let $U=\{1,..., N\}$ denote the target population consisting of $N$ labelled units. Each unit $i$ has an associated vector of auxiliary variables $\bx_{i}$ (a realisation of the random vector $\bX_{i}$ in the super-population) and the study variable $y_{i}$ (a realisation of the random variable $Y_{i}$ in the super-population). Let $\{ (y_i, \bx_i), i \in S_A\}$ be a dataset of a non-probability sample of size $n_A$ and let  $\{\left(\bx_i, \pi_{i}\right), i \in S_B\}$ be a dataset of a probability sample of size $n_B$, where only information about variables $\bX$ and inclusion probabilities $\pi$ (which in the super population model are also considered to be random variables) are available. Let $R_i$ be an indicator of inclusion into non-probability sample. Each unit in the sample $S_B$ has been assigned a~design-based weight given by $d_i = 1/\pi_i$.

The goal is to estimate a~finite population mean $\displaystyle\mu_{y}=\frac{1}{N}\sum_{i=1}^{N} y_{i}$ of the target variable $Y$. As values of $y_{i}$ are not observed in the probability sample, it cannot be used to estimate the target quantity. Instead, one could try combining the non-probability and probability samples to estimate $\mu_{y}$. In this paper we do not consider modifications for the possibly occurring overlap. The above description of the data is presented in a more concise form in Table \ref{tab:tabela1_1}.


\begin{table}
\centering
\captionsetup{aboveskip=0pt, belowskip=0pt} % Reduce space around caption
\scriptsize % Use \scriptsize or \tiny for smaller font size
\setlength{\tabcolsep}{3pt} % Reduce horizontal padding between columns
\renewcommand{\arraystretch}{0.8} % Reduce vertical padding between rows
\caption{Two Sample Setting}
% \vspace{+3mm} % Remove or adjust to save space
\label{tab:tabela1_1}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|>{\centering\arraybackslash}p{3cm}|c|>{\centering\arraybackslash}p{2.5cm}|}
\hline 
Sample & & 
\begin{tabular}{c} % Changed from {l} to {c}
Auxiliary Variables \\
$\boldsymbol{X}$
\end{tabular}
& 
\begin{tabular}{c} % Changed from {c} to {c} (already centered)
Target Variable \\
$Y$
\end{tabular}
& 
\begin{tabular}{c} % Changed from {l} to {c}
Design $(d)$ Weights
\end{tabular}
\\
\hline 
\multirow{3}{*}{$S_A$ (non-probability)} 
& 1 & $\checkmark$ & $\checkmark$ & ? \\
\cline{2-5}
& $\ldots$ & $\checkmark$ & $\checkmark$ & ? \\
\cline{2-5}
& $n_A$ & $\checkmark$ & $\checkmark$ & ? \\
\hline 
\multirow{3}{*}{$S_B$ (probability)} 
& $n_A+1$ & $\checkmark$ & ? & $\checkmark$ \\
\cline{2-5}
& $\ldots$ & $\checkmark$ & ? & $\checkmark$ \\
\cline{2-5}
& $n_A+n_B$ & $\checkmark$ & ? & $\checkmark$ \\
\hline
\end{tabular}%
} % End of \resizebox
\end{table}


## Mass Imputation estimators
Imputation refers to the process of replacing missing or incomplete data with substituted values. The goal of imputation is to allow for more complete data analysis, as many statistical methods require complete datasets. Common imputation techniques include:
\begin{itemize}
    \item Mean imputation: where missing values are replaced by the mean of the observed data for that variable.
    \item Median imputation: where the median value of the observed data is used.
    \item Regression imputation: where missing values are estimated based on a regression model built from other available data.
\end{itemize}

Imputation helps prevent data bias and maintains dataset size, ensuring that missing data points do not skew analysis results. It is particularly useful when data is missing at random or when only a small portion of the data is missing.

Mass imputation is the application of imputation techniques to entire datasets where many observations have missing values for the given variable. @kim_combining_2021, @yang2021integration, @Beres propose the following imputation strategies as:
\begin{itemize}
    \item Model based approach (GLM),
    \item Nearest neigbour imputation (NN),
    \item Predictive mean mathing (PMM).
\end{itemize}

Mass imputation is particularly useful in large datasets where missing data can be widespread, and it seeks to preserve the relationships between variables, thus improving the overall integrity of the data.

By assumptions (Table \ref{tab:tabela1_1}), we do not know the value of the dependent variable $Y$ for the units in the probability sample. In this case, the method will be to impute the values of the explanatory variable for all units in the probability sample. We therefore treat the non-probability sample as a training set that is used to build the imputation model. In this subsection, we distinguish three main methods of mass imputation based on linear models and the k-nearest neighbours algorithm. Other popular methods for estimating the variable $Y$ from the variable $\bX$ can also be considered, e.g. machine learning models such as random forests or neural networks. 

We can obtain an estimate of the population mean based on known design weights and an imputation model for units from the probability sample:

\begin{equation}
\hat{\mu}_{M I}=\frac{1}{\hat{N}_{\mathrm{B}}} \sum_{i \in S_{\mathrm{B}}} d_i^{\mathrm{B}} \hat{y}, 
\end{equation}
 $\hat{N}_{\mathrm{B}} = \sum_{i \in S_B} d_i^B$ and $\hat{y}$ is the estimated value of $y$ for units from probability samples based on mass imputation model.
 
  This estimator can be understood as a version of the Horvitz--Thompson estimator, which are used to estimate mean or total values in the population (based on probability sampling and inclusion probabilities). The only difference is that in our case, instead of the known values of the $Y$ variable, we use its estimated equivalents.
  
### Generalized Linear Models

Let us assume the following parametric model for the sample $S_A$ based on the conditional expected value of the variable $Y$. Let
\begin{equation}
\mathbb{E}\left(y_i \mid \boldsymbol{x}_i\right)=m\left(\boldsymbol{x}_i, \boldsymbol{\beta}_0\right)
\end{equation}
for a certain $p$--dimensional vector $\boldsymbol{\beta}_0$ and a known $m$ function from a given class of mean functions for generalized linear models for which we can distinguish the following cases
\begin{enumerate}
    \item \textbf{\( Y \) variable is continous (linear model):}
    \begin{align*}
        m\left(\boldsymbol{x}_i, \boldsymbol{\beta}_0\right) &= \boldsymbol{x}_i^T \boldsymbol{\beta}_0
    \end{align*}

    \item \textbf{The variable \( Y \)  represents a discrete variable (\textit{count data}):}
    \begin{align*}
        m\left(\boldsymbol{x}_i, \boldsymbol{\beta}_0\right) &= \exp \left(\boldsymbol{x}_i^T \boldsymbol{\beta}_0\right)
    \end{align*}

    \item \textbf{The variable \( Y \)  is binary (logistic model):}
    \begin{align*}
        m\left(\boldsymbol{x}_i, \boldsymbol{\beta}_0\right) &= \frac{\exp \left(\boldsymbol{x}_i^T \boldsymbol{\beta}_0\right)}{\exp \left(\boldsymbol{x}_i^T \boldsymbol{\beta}_0\right) + 1}
    \end{align*}
\end{enumerate}

According to the model described, we have 
$$
y_i=m\left(\boldsymbol{x}_i\right)+\varepsilon_i, \quad i=1,2, \ldots, N.
$$
We also assume that the random variables $\varepsilon_i$ are independent with $\mathbb{E} \left(\varepsilon_i \right) = 0$ and $\sigma^2 \left(\varepsilon_i \right) = \mathbf{v} \left(\bx\right) \sigma^2$. It is assumed that $\mathbf{v} \left(\bx\right)$ has a known value and is homogeneous, i.e. homogeneous, regardless of the sample under study. Let us represent the process of mass imputation of a linear model to a sample $S_B$. Finally we are interesting in finding such $\bbeta$ that is the solution of the following equation
\begin{equation}
\label{eq-2.3}
U(\boldsymbol{\beta})=\frac{1}{n_A} \sum_{i \in S_A}\left\{y_i-m\left(\bx_i ; \boldsymbol{\beta}\right)\right\} h\left(\bx_i ; \boldsymbol{\beta}\right)=\mathbf{0},
\end{equation}

for some $p$-dimensional vector of function $h\left(\bx_i ; \boldsymbol{\beta}\right)$, where $h\left(\bx_i ; \boldsymbol{\beta}\right) = \bx_i$ might be used for certain applications. The mass imputation process is described in Algorithm \ref{algo-1}.
\begin{algorithm}[H]
\caption{Mass imputation based on a generalized linear model}
\label{algo-1}
\begin{algorithmic}[1]
 \State Estimate the regression model $\mathbb{E}[Y|\boldsymbol{X}=\boldsymbol{x}]=m(\boldsymbol{x}, \boldsymbol{\beta})$\ basing on units from $S_A$ sample.
 \State For each $i \in S_B$, calculate the imputed value as
 $$
 \hat{y}_i = m\left(\boldsymbol{x}_{i},\hat{\boldsymbol{\beta}}\right).
 $$
\end{algorithmic}
\end{algorithm}

## Nearest Neighbour Algorithm

On the other hand, it is also possible to consider a non-parametric model for the problem described, i.e. for each individual from sample $S_B$, the k-nearest neighbours from sample $S_A$ are found based on the values of the auxiliary vector $\bX$ and the corresponding metric. Then, the missing values of the variable $Y$ from sample $S_B$ are replaced by the values (or their mean if more than one neighbour is considered) of this variable for the corresponding neighbours from sample $S_A$. The algorithm is as follows

\begin{algorithm}[H]
\caption{Mass imputation using the k-nearest-neighbour algorithm}
\label{algo-2}
\begin{algorithmic}[1]
\State If $k=1$, then for each $i \in S_B$ match $\hat{\nu}(i)$ such that
$\displaystyle \hat{\nu}(i)=
\argmin_{j\in S_{A}}d\left(\bx_i,\bx_j\right)$.
\State If $k>1$, then
$$\hat{\nu}(i, z) = \argmin_{\displaystyle j\in S_{A}\setminus\bigcup_{t=1}^{z-1}
\{\hat{\nu}(i, t)\}} d\left(\bx_i, \bx_j\right)$$
i.e. $\hat{\nu}(i, z)$ is $z$-th nearest neighbour from the sample.\;
\State For each $i \in S_B$, calculate the imputed value as
$$
\hat{y}_i = \frac{1}{k}\sum_{t=1}^{k}y_{\hat{\nu}(i, t)}.
$$
\end{algorithmic}
\end{algorithm}

Note that the algorithm differs depending on the number of nearest neighbours chosen. In case $k=1$ the nearest neighbour value is imputed according to the chosen metric, for example the Euclidean metric. In case $k>2$ the average of the nearest neighbours values is imputed. The literature indicates that this method suffers from the so-called curse of multidimensionality, i.e. for samples with several explanatory variables, imputation can lead to a large variance in the estimator. On the other hand, the algorithm is easy to interpret and simple to implement. 

## Predictive Mean Matching

Predictive mean-matching imputation is a particularly well-known way of dealing with non-response among respondents, and is favoured by statistical offices for compiling a country's official population statistics. It is a version of the k-nearest neighbour algorithm, but instead of looking at the distances between the vectors of the auxiliary variables, it looks at the distance between the functions of the mean vectors. This helps to reduce the curse of multidimensionality and, at the same time, allows the observed values of the explanatory variable or their mean to be calculated.
Let us therefore present two algorithms that describe the steps to follow to perform a mass imputation using the mean matching method.

\begin{algorithm}[H]
\caption{$\hat{y}-\hat{y}$ Imputation:}
\label{algo-3}
\begin{algorithmic}[1]
\State Estimate regression model $\mathbb{E}[Y|\boldsymbol{X}=\boldsymbol{x}]=m(\boldsymbol{x}, \boldsymbol{\beta})$.\;
\State Impute $$\hat{y}_{i}=m\left(\boldsymbol{x}_{i},\hat{\boldsymbol{\beta}}\right), 
\hat{y}_{j}=m\left(\boldsymbol{x}_{j},\hat{\boldsymbol{\beta}}\right)$$
for $i\in S_{B}, j\in S_{A}$ and assign each 
$i\in S_{B}$ to $\hat{\nu}(i)$, where
$$\displaystyle \hat{\nu}(i)=
\argmin_{j\in S_{A}}\lVert \hat{y}_{i}-\hat{y}_{j}\rVert$$ or
$$\displaystyle \hat{\nu}(i)=
\argmin_{j\in S_{A}}d\left(\hat{y}_{i},\hat{y}_{j}\right)$$ if $d$ is not induced by the norm.\;

\State If $k>1$, then:
$$\hat{\nu}(i, z) = \argmin_{\displaystyle j\in S_{A}\setminus\bigcup_{t=1}^{z-1}
\{\hat{\nu}(i, t)\}} d\left(\hat{y}_{i},\hat{y}_{j}\right)$$
e.g., $\hat{\nu}(i, z)$ is $z$-th nearest neighbor from a sample.\;
\State For $i \in S_B$, calculate imputation value as 
$$
\hat{y}_i = \frac{1}{k}\sum_{t=1}^{k}y_{\hat{\nu}(i, t)}.
$$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{$\hat{y}-y$ Imputation:}
\label{algo-4}
\begin{algorithmic}[1]
\State Estimate regression $\mathbb{E}[Y|\boldsymbol{X}=\boldsymbol{x}]=m(\boldsymbol{x}, \boldsymbol{\beta})$.\;
\State Impute $\hat{y}_{i}=m\left(\boldsymbol{x}_{i},\hat{\boldsymbol{\beta}}\right)$ 
for $i \in S_{B}$ and assign each 
$i \in S_{B}$ do $\hat{\nu}(i)$, where
$\displaystyle \hat{\nu}(i)=
\argmin_{j \in S_{A}}\lVert \hat{y}_{i}-y_{j}\rVert$ or
$\displaystyle \hat{\nu}(i)=
\argmin_{j \in S_{A}}d\left(\hat{y}_{i},y_{j}\right)$ 
if $d$ not induced by the norm.\;
\State If $k>1$, then:
$$\hat{\nu}(i, z) = \argmin_{\displaystyle j \in S_{A} \setminus \bigcup_{t=1}^{z-1}
\{\hat{\nu}(i, t)\}}
d\left(\hat{y}_{i},y_{j}\right).$$
\State For each $i \in S_B$ calculate imputation value as
$$
\hat{y}_i = \frac{1}{k}\sum_{t=1}^{k}y_{\hat{\nu}(i, t)}.
$$
\end{algorithmic}
\end{algorithm}

As can be seen, the difference between the two algorithms is due to step 2. In the first approach, we compare $\hat{y}$ from samples $S_A$ and $S_B$. The second, on the other hand, compares $\hat{y}$ from sample $S_B$ with the known $y$ from sample $S_A$. It is worth noting that proof of the consistency of these estimators can be found in \citet{Beres}.

## Inverse Probability Weighting estimators

The main disadvantage of non-probability sampling is the unknown selection mechanism for a unit to be included in the sample. This is why we talk about the so-called ``biased sample'' problem. The inverse probability approach is based on the assumption that a reference probability sample is available and therefore we can estimate the propensity score of the selection mechanism. In recent years, a number of articles have addressed this issue. @chen2020doubly propose maximum likelihood estimation approach for estimating propensity scores for selection mechanism. @wu2022statistical present the approach based on generalized estimating equations, this method is also mentioned in 
@yang_doubly_2020. On the other hand calibration approach for quantiles was explained @beresewicz2024inference and @santanna_covariate_2022 present the approach based on maximize the covariate distribution balance among different treatment groups.

In the formal framework, let us introduce the following assumptions for propensity score model, which will imply a number of properties derived in the thesis.
\begin{itemize}
    \item[(A1)] The selection indicator $R_i^A$ and explanatory variable $y_i$ are independent.
    \item[(A2)]All units have a so-called non-probability sample propensity score, which is non-zero, i.e. $\pi_i^{\mathrm{A}} > 0$, where $\pi_i^{\mathrm{A}} = P_q\left(R_i^A=1 \mid \bx_i, y_i\right)$, where $q$ refers to the model for the selection mechanism for the non-probability sample (propensity score model).
    \item[(A3)] Indicator variables $R_i^A$ and $R_j^A$ are independent with $i \neq j$. 
\end{itemize}

The estimated propensity score is used to construct an inverse probability weighting estimator of the population mean of the form

\begin{equation}
\begin{gathered}
\hat{\mu}_{I P W}=\frac{1}{\hat{N}^A} \sum_{i \in S_A} \frac{y_i}{\hat{\pi}_i^A}.
\end{gathered}
\end{equation}
where $\hat{N}^A = \sum_{i \in S_A} \hat{d}_i^A = \sum_{i \in S_A} \frac{1}{\hat{\pi}_i^A}$.

### Maximum Likelihood Estimation
Consider the following likelihood function
\begin{align}
    \begin{split}
 \ell(\boldsymbol{\theta}) & =\sum_{i=1}^N\left\{R_i^A \log \pi_i^{\mathrm{A}}+\left(1-R_i^A\right) \log \left(1-\pi_i^{\mathrm{A}}\right)\right\} \\ & =\sum_{i \in S_{\mathrm{A}}} \log \left\{\frac{\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)}{1-\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)}\right\}+\sum_{i=1}^N \log \left\{1-\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)\right\}
    \end{split}
\end{align}
In practice, a function of this form cannot be used because we do not observe all units from the population. Hence, the second component of the function is replaced by the Horvitz-Thompson estimator, which is used when having access to the design weights for the units in the sample. In our case, these will be the weights $d_i^B$ for the units in the sample $S_B$. 
We then have
\begin{equation}
\ell^*(\boldsymbol{\theta})=\sum_{i \in S_{\mathrm{A}}} \log \left\{\frac{\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)}{1-\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)}\right\}+\sum_{i \in S_{\mathrm{B}}} d_i^{\mathrm{B}} \log \left\{1-\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)\right\}.
\end{equation}
Our objective is to find the maximum likelihood estimator $\hat{\pi}_{i}^{A} = \pi(\bx_{i}, \hat{\btheta})$, such that $\hat{\btheta}$ maximises the function defined above.

### Generalized Estimating Equations

Equations of the type $\operatorname{U}(\btheta) = \bZero$, where $\operatorname{U}(\btheta) = \frac{\partial}{\partial \btheta} l^*(\btheta)$, obtained from the maximum likelihood estimation can be replaced by a system of generalized estimating equations of the form
\begin{equation}
\label{gee}
\mathbf{G}(\btheta)=\sum_{i \in S_A} h\left(\bx_i, \btheta\right)-\sum_{i \in S_B} d_i^B \pi\left(\bx_i, \btheta\right) h\left(\bx_i, \btheta\right) = \bZero,
\end{equation}
where $h\left(\bx_i, \btheta\right)$ is a certain continuous function. In the literature, the most commonly considered functions are $h\left(\bx_i, \btheta\right) = \bx_i$ and $h\left(\bx_i, \btheta\right) = \bx_i \pi\left(\bx_i, \btheta\right)^{-1}$. Note that if the function $h$ is equal to the vector of observed characteristics $\bx$, then $\mathbf{G}$ is reduced to
$$
\mathbf{G}(\btheta) = \sum_{i \in S_A} \boldsymbol{x}_i-\sum_{i \in S_B} d_i^B \pi\left(\bx_i, \boldsymbol{\theta}\right) \boldsymbol{x}_i.
$$
In the next subsection we will proof that this is disorted version of 
MLE approach with $\pi_i^A$ modelling by logistic regression.
If we use the second form of the function $h$ we get the following form of the function $\mathbf{G}$
$$
\mathbf{G}(\btheta) = \sum_{i \in S_A} \frac{\boldsymbol{x}_i}{\pi\left(\bx_i, \boldsymbol{\theta}\right) }-\sum_{i \in S_B} d_i^B \boldsymbol{x}_i.
$$

The advantage of this method is the ability to estimate with global values of the variables (e.g. from external sources) instead of a probability sample. Note that this is allowed by the second form of the $\mathbf{G}$ function. Its second term is nothing more than the estimated sums of the $\bx$ variables. On the other hand, empirical studies suggest that the process of solving this type of equation may be less stable than the maximum likelihood method. In other words, the iterative algorithm for finding zeros that satisfy equation (\ref{gee}) may not converge. 

In the `nonprobsvy` package the propensity scores can modelled using three different link functions: logistic, complementary log-log and probit. The logistic regression is the most commonly used for modelling probabilities. This method is based on the so-called sigmoidal function and for our settign it has the form
\begin{align}
\label{logit-fun}
    \pi_{i}^{A} = \pi(\bx_{i}, \btheta) = \frac{\exp\left(\bx_{i}^{\top}\btheta_{0}\right)}{ 1 + \exp\left(\bx_{i}^{\top}\btheta_{0}\right)}
\end{align}
It satisfies certain properties to model the probability. For our scheme, this will be the probability of belonging to the non-probability sample.

Another approach to modelling binary variables and also probabilities is probit regression. It is based on the standard normal distribution. Probability density function is given by the following function
\begin{displaymath}
    \pdv{\Phi(t)}{t}=\phi(t)=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{1}{2} t^2\right)
\end{displaymath}
and the probability is modelled as
\begin{align*}
    \pi_{i}^{A} = \pi(\bx_{i},\btheta) = \Phi(\bx_{i}^{\top}\btheta).
\end{align*}

Regression with the cloglog model is particularly useful if we are modelling rare phenomena, i.e. the probabilities will oscillate around the values 1 and 0. Compared to the sigmoidal function and the distribution, the cloglog function is more asymmetric towards the value 0.5. The model can be written as
\begin{align*}
    \pi_i^A = \pi(\bx_{i}, \btheta) = 1 - \exp(-\exp(\bx_{i}^{\top}\btheta)).
\end{align*}

\begin{table}[H]
\small
\centering
\begin{tabular}{p{2cm} p{6.5cm} p{6.5cm}}
\toprule
Link & MLE Function & Gradient \\ \midrule

\code{logit} & 
$\displaystyle
\begin{aligned}
& \sum_{i \in S_{\mathrm{A}}} \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \\
& {} - \sum_{i \in S_{\mathrm{B}}} d_i^{\mathrm{B}} \log \left[ 1 + \exp \left( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \right) \right]
\end{aligned}
$ & 
$\displaystyle
\begin{aligned}
& \sum_{i \in S_{A}} \boldsymbol{x}_{i} \\
& {} - \sum_{i \in S_{B}} d_{i}^{B} \pi(\boldsymbol{x}_{i}, \boldsymbol{\theta}) \boldsymbol{x}_{i}
\end{aligned}
$ \\ \midrule

\code{probit} & 
$\displaystyle
\begin{aligned}
& \sum_{i \in S_{A}} \log \left( \frac{ \Phi( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} ) }{ 1 - \Phi( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} ) } \right) \\
& {} + \sum_{i \in S_{B}} d_{i}^{B} \log \left[ 1 - \Phi( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} ) \right]
\end{aligned}
$ & 
$\displaystyle
\begin{aligned}
& \sum_{i \in S_A} \frac{ \phi( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} ) }{ \Phi( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} ) [ 1 - \Phi( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} ) ] } \boldsymbol{x}_i \\
& {} - \sum_{i \in S_B} d_i^B \frac{ \phi( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} ) }{ 1 - \Phi( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} ) } \boldsymbol{x}_i
\end{aligned}
$ \\ \midrule

\code{cloglog} & 
$\displaystyle
\begin{aligned}
& \sum_{i \in S_{A}} \left\{ \log \left[ 1 - \exp \left( -\exp( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} ) \right) \right] \right\} \\
& {} + \exp( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} ) - \sum_{i \in S_{B}} d_{i}^{B} \exp( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} )
\end{aligned}
$ & 
$\displaystyle
\begin{aligned}
& \sum_{i \in S_{A}} \frac{ \exp( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} ) \boldsymbol{x}_{i} }{ \pi( \boldsymbol{x}_{i}, \boldsymbol{\theta} ) } \\
& {} - \sum_{i \in S_{B}} d_{i}^{B} \exp( \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta} ) \boldsymbol{x}_{i}
\end{aligned}
$ \\ \bottomrule

\end{tabular}
\caption{MLE Functions and Gradients for Different Link Functions}
\end{table}

\begin{table}[H]
\small
\centering
\begin{tabular}{p{2cm} p{6.5cm} p{6.5cm}}
\toprule
Link & GEE Jacobian \( h = \boldsymbol{x}_i \) & GEE Jacobian \( h = \boldsymbol{x}_i \pi\left( \boldsymbol{x}_i, \boldsymbol{\theta} \right)^{-1} \) \\ \midrule

\code{logit} & 
$\displaystyle
\begin{aligned}
& - \sum_{i \in S_B} \frac{1}{\pi_i^B} \, \pi_i^A\left( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \right) \left( 1 - \pi_i^A\left( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \right) \right) \boldsymbol{x}_i \boldsymbol{x}_i^{\top}
\end{aligned}
$ & 
$\displaystyle
\begin{aligned}
& -\sum_{i \in S_A} \frac{ 1 - \pi_i^A\left( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \right) }{ \pi_i^A\left( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \right) } \boldsymbol{x}_i \boldsymbol{x}_i^{\top}
\end{aligned}
$ \\ \midrule

\code{probit} & 
$\displaystyle
\begin{aligned}
& - \frac{1}{N} \sum_{i \in S_B} \frac{ \dot{\pi}_i^A\left( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \right) }{ \pi_i^B } \boldsymbol{x}_i \boldsymbol{x}_i^{\top}
\end{aligned}
$ & 
$\displaystyle
\begin{aligned}
& - \frac{1}{N} \sum_{i \in S_A} \frac{ \dot{\pi}_i^A\left( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \right) }{ \left( \pi_i^A\left( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \right) \right)^2 } \boldsymbol{x}_i \boldsymbol{x}_i^{\top}
\end{aligned}
$ \\ \midrule

\code{cloglog} & 
$\displaystyle
\begin{aligned}
& - \frac{1}{N} \sum_{i \in S_B} \frac{ 1 - \pi_i^A\left( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \right) }{ \pi_i^B } \exp \left( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \right) \boldsymbol{x}_i \boldsymbol{x}_i^{\top}
\end{aligned}
$ & 
$\displaystyle
\begin{aligned}
& - \frac{1}{N} \sum_{i \in S_A} \frac{ 1 - \pi_i^A\left( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \right) }{ \left( \pi_i^A\left( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \right) \right)^2 } \exp \left( \boldsymbol{x}_i^{\top} \boldsymbol{\theta} \right) \boldsymbol{x}_i \boldsymbol{x}_i^{\top}
\end{aligned}
$ \\ \bottomrule
\end{tabular}
\caption{GEE Jacobians for Different Link Functions}
\end{table}

## Doubly Robust estimators

The inverse probability weighting and mass imputation estimators are sensible on misspecified models for propensity score and outcome variable respectively. For this purpose so called doubly-robust methods, which take into account these problems, are presented.

The proposed estimation procedure addresses the challenge of combining data from nonprobability and probability survey samples. Traditional semiparametric models, often applied to such problems, are not directly usable in this context due to the distinct nature of the two samples. Instead, a joint randomization framework is employed, integrating semiparametric models for propensity scores with outcome regression for the nonprobability sample and design-based inference from the probability sample. This framework leads to a doubly robust (DR) estimation approach, which is effective in the presence of model misspecifications.

Inverse Probability Weighted (IPW) estimators are sensitive to misspecified propensity score models, particularly when propensity scores are very small. To improve robustness and efficiency, the doubly robust method incorporates a prediction model for the response variable. Moreover, even if one of the models is misspecified, the DR estimator remains consistent, showcasing the ``double robustness'' property.

### Joint Randomization Approach

The joint randomization approach combines two processes: the selection mechanism of a non-probability sample, modelled by propensity scores, and the design-based inference from a probability sample.

The response $y_i$ is predicted using a regression model $m(\bx_i, \bbeta)$ (or NN/PMM methods), where $\bbeta$ is estimated from the non-probability sample. With known design weights $d_i^B$ for $i \in S_B$ we can define the DR estimator as
\begin{equation}
\label{dr}
\hat{\mu}_{\mathrm{DR}}=\frac{1}{\hat{N}^{\mathrm{A}}} \sum_{i \in S_{\mathrm{A}}} d_i^{\mathrm{A}}\left\{y_i-m\left(\boldsymbol{x}_i, \hat{\boldsymbol{\beta}}\right)\right\}+\frac{1}{\hat{N}^{\mathrm{B}}} \sum_{i \in S_{\mathrm{B}}} d_i^{\mathrm{B}} m\left(\boldsymbol{x}_i, \hat{\boldsymbol{\beta}}\right),
\end{equation}
where $d_i^A=\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)^{-1}, \hat{N}^A=\sum_{i \in S_A} d_i^A$ and $\hat{N}^B=\sum_{i \in S_B} d_i^B$.

It remains consistent if either the propensity score model $\pi(\bx_i, \btheta)$ or the outcome regression model $m(\bx_i, \bbeta)$ is correctly specified.

The joint randomization approach ensures robustness by accounting for randomness in both the non-probability sample through $\pi(\bx_i, \btheta)$ and the probability sample through design-based inference.

### Minimization of the bias for doubly robust methods

By reducing the variance of the estimators, for example by variable selection, we cannot control the bias of the estimator, which may increase. Therefore, according to @yang_doubly_2020, the idea is to determine the equations leading to the estimation of the $\bbeta$ and $\btheta$ parameters based on the bias of the population mean estimator. In contrast to the joint randomization approach, this method allows for the estimation of the parameters $\boldsymbol{\beta}$ and $\boldsymbol{\theta}$ in a single step, rather than in two separate steps.

We will first present the bias of the doubly robust estimator and then, using optimisation techniques, discuss the equations leading to its minimization. Thus we have
\begin{equation}
\begin{aligned}
\operatorname{bias}\left(\hat{\mu}_{D R}\right) & = \mid\hat{\mu}_{DR}-\mu\mid& \\
& =\frac{1}{N} \sum_{i=1}^N\left\{\frac{R_i^A}{\pi_i^A\left(\boldsymbol{x}_i^{\mathrm{T}} \boldsymbol{\theta}\right)}-1\right\}\left\{y_i-m\left(\boldsymbol{x}_i^{\mathrm{T}} \boldsymbol{\beta}\right)\right\}\\
& + \frac{1}{N} \sum_{i=1}^N\left(R_i^B d_i^B-1\right) m\left(\boldsymbol{x}_i^{\mathrm{T}} \boldsymbol{\beta}\right)
\end{aligned}
\end{equation}

To minimize $\operatorname{bias}\left(\hat{\mu}_{D R}\right)^2$ let us calculate the gradient of the square of the bias at $\left(\bbeta, \btheta\right)$. We then have 

\begin{equation*}
\frac{\partial \operatorname{bias}\left(\hat{\mu}_{D R}\right)^2}{\partial\left(\boldsymbol{\beta}^{\mathrm{T}}, \boldsymbol{\theta}^{\mathrm{T}}\right)^{\mathrm{T}}}=2 \operatorname{bias}\left(\hat{\mu}_{D R}\right) J(\theta, \beta),
\end{equation*}
where
\begin{equation*}
J(\theta, \beta)=\left(\begin{array}{l}
J_1(\theta, \beta) \\
J_2(\theta, \beta)
\end{array}\right)=\left(\begin{array}{c}
\sum_{i=1}^N R_i^A\left\{\frac{1}{\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)}-1\right\}\left\{y_i-m\left(\boldsymbol{x}_i, \boldsymbol{\beta}\right)\right\} \boldsymbol{x}_i \\
\sum_{i=1}^N \frac{R_i^A}{\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)} \frac{\partial m\left(\boldsymbol{x}_i, \boldsymbol{\beta}\right)}{\partial \boldsymbol{\beta}}-\sum_{i \in S_{\mathrm{B}}} d_i^{\mathrm{B}} \frac{\partial m\left(\boldsymbol{x}_i, \boldsymbol{\beta}\right)}{\partial \boldsymbol{\beta}}
\end{array}\right),
\end{equation*}
which leads to the problem of solving the following system of equations
\begin{equation}
\label{bias-min}
    \left(\begin{array}{c}
\sum_{i=1}^N R_i^A\left\{\frac{1}{\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)}-1\right\}\left\{y_i-m\left(\boldsymbol{x}_i, \boldsymbol{\beta}\right)\right\} \boldsymbol{x}_i \\
\sum_{i=1}^N \frac{R_i^A}{\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)} \frac{\partial m\left(\boldsymbol{x}_i, \boldsymbol{\beta}\right)}{\partial \boldsymbol{\beta}}-\sum_{i \in S_{\mathrm{B}}} d_i^{\mathrm{B}} \frac{\partial m\left(\boldsymbol{x}_i, \boldsymbol{\beta}\right)}{\partial \boldsymbol{\beta}}
\end{array}\right) = \bZero,
\end{equation}
which can be solved using Newton--Raphson optimization method.

# Package contents and implementation

All of the methods described in this paper have been implemented in the R package \texttt{nonprobsvy} @nonprobsvy. In this chapter, we will show you how to use the main \texttt{nonprob} function of the package and what its main features are. The package has been written to be as compatible as possible with the survey package for probablistic inference. Namely, the first step to use the nonprobsvy package is to define an object using the svydesign function that stores the probability sample \texttt{data.frame} and other objects, such as design weights. This is a negligible step if, instead of the probability sample, we have access to the values of the vector of sums of variables in the population. It is also worth mentioning that in order to speed up the calculations in the case of variable selection, part of the package, or more precisely the whole variable selection algorithm, was written in \texttt{C++} using the \texttt{Rcpp} (@Rcpp) package, which allows the C++ code to be called in the R environment. Moreover, the package is supported by other R packages such as \texttt{foreach} @foreach (looping construct), \texttt{maxLik} @maxLik (maximum likelihood estimation), \texttt{Matrix} @Matrix (matrix operations), \texttt{MASS} @MASS (statistical functions and datasets), \texttt{ncvreg} @ncvreg (regularization methods), \texttt{mathjaxr} @mathjaxr (rendering equations in documentation), \texttt{nleqslv} @nleqslv (solving nonlinear equations), and \texttt{doParallel} @doParallel (parallel computing).

## Usage


```{r, eval=FALSE}
nonprob(
  data,
  selection = NULL,
  outcome = NULL,
  target = NULL,
  svydesign = NULL,
  pop_totals = NULL,
  pop_means = NULL,
  pop_size = NULL,
  method_selection = c("logit", "cloglog", "probit"),
  method_outcome = c("glm", "nn", "pmm"),
  family_outcome = c("gaussian", "binomial", "poisson"),
  subset = NULL,
  strata = NULL,
  weights = NULL,
  na_action = NULL,
  control_selection = controlSel(),
  control_outcome = controlOut(),
  control_inference = controlInf(),
  start_selection = NULL,
  start_outcome = NULL,
  verbose = FALSE,
  x = TRUE,
  y = TRUE,
  se = TRUE,
  ...
)
```

## Arguments

Below is the definition of most of the arguments we can pass to the function. These are described in more detail in the documentation on the [CRAN](https://CRAN.R-project.org/package=nonprobsvy) platform.

\begin{table}[H]
\centering
\begin{tabular}{p{3cm}p{8cm}p{3cm}}
\hline
Argument & Description & Default \\
\hline
    \code{data} & \code{data.frame} with data from the non-probability sample. & - \\
    \code{selection} & \code{formula} for the selection (propensity) equation. & NULL \\
    \code{outcome} & \code{formula} for the outcome equation. & NULL \\
    \code{target} & \code{formula} with target variables. & NULL \\
    \code{svydesign} & Optional \code{svydesign} object containing probability sample and design weights. & NULL \\
    \code{pop\_totals} & Optional named \code{vector} with population totals of the covariates. & NULL \\
    \code{pop\_means} & Optional named \code{vector} with population means of the covariates. & NULL \\
    \code{pop\_size} & Optional \code{double} with population size. & NULL \\
    \code{method\_selection} & Character string specifying the method for propensity score estimation (e.g., "logit"). & "logit" \\
    \code{method\_outcome} & Character string specifying the method for response variable estimation (e.g., "glm"). & "glm"\\
    \code{family\_outcome} & Character string describing the error distribution and link function to be used in the model (e.g., "gaussian"). & "gaussian" \\
    \code{subset} & Optional \code{vector} specifying a subset of observations to be used in the fitting process. & NULL\\
    \code{strata} & Optional \code{vector} specifying strata. & NULL\\
    \code{weights} & Optional \code{vector} of prior weights to be used in the fitting process. & NULL\\
    \code{na\_action} & \code{function} indicating what should happen when the data contain NAs. & NULL\\
    \code{control\_selection} & \code{list} indicating parameters to use in fitting selection model for propensity scores. & controlSel()\\
    \code{control\_outcome} & \code{list} indicating parameters to use in fitting model for outcome variable. & controlOut() \\
    \code{control\_inference} & \code{list} indicating parameters to use in inference based on probability and non-probability samples. & controlInf() \\
    \code{start\_selection} & Optional \code{vector} with starting values for the parameters of the selection equation. & NULL \\
    \code{start\_outcome} & Optional \code{vector} with starting values for the parameters of the outcome equation. & NULL \\
    \code{verbose} & Logical value indicating if verbose output should be printed. & FALSE \\
    \code{x} & Logical value indicating whether to return the model matrix of covariates as part of the output. & TRUE \\
    \code{y} & Logical value indicating whether to return the vector of outcome variable as part of the output. & TRUE \\
    \code{se} & Logical value indicating whether to calculate and return the standard error of the estimated mean. & FALSE \\
    \code{...} & Additional optional arguments. & \\
\hline
\end{tabular}
\caption{\code{nonprob} function arguments description}
\label{tab-arguments-nonprob}
\end{table}

In addition to using the survey package for design-based inference when probability samples are available, it also supports the various methods for estimating propensity scores and outcome models described in this thesis, such as logistic regression, complementary log-log models, probit models, generalized linear models, nearest neighbour algorithms and predictive mean matching.

After this neat description of the main functionality of the package, we will move on to some examples of its use. We will show how to define the given arguments in order to obtain estimates of interest as a result. We will be less interested in the results than in the way they are presented. There will be room in the following chapters for an analysis of simulations and applications of the package to the real world. We will focus on the three main estimators, as function calls for other functionalities such as variable selection, other linking functions or mass imputation methods.

Suppose we have two data sets, the first \textit{nonprob} containing individuals from the non-probability sample. As assumed, this set contains information on $k$ variables $\bold{x}$, e.g. sex, income, etc., and the explanatory variable $y$. In addition, there is a probability sample defined using the survey package and the svydesign function, containing design weights and $\bold{x}$ variables, but no $y$ variable. 

In the case of a \textbf{mass imputation} estimator, the function should be defined as follows

```{r, eval=FALSE}
nonprob(
  outcome = y ~ x1 + x2 + ... + xk, 
  data = nonprob, 
  svydesign = prob, 
  method_outcome = "glm", 
  family_outcome = "gaussian"
)
```

As can be seen, we have defined a formula for the imputation of the explanatory variable similar to the function glm. We have also specified the datasets in the arguments data (non-probability sample) and svydesign (probability sample). Finally, we have provided information about the mass imputation method (glm) and the type of explanatory variable (continuous variable). Let us now look at the \textbf{propensity score} estimator

```{r, eval=FALSE}
nonprob(
  selection =  ~ x1 + x2 + ... + xk, 
  target = ~ y, 
  data = nonprob, 
  svydesign = prob, 
  method_selection = "logit"
)
```

As you can see, three new arguments have appeared - \texttt{selection} and \texttt{target} are responsible for the formulas for modelling the inclusion model and for defining the variable for which we calculate the population mean. In addition, in the \texttt{method\_selection} argument, we specify the name of the link function that models the probability of inclusion in the non-probability sample. For the \textbf{doubly robust estimator} call it is as follows

```{r,eval=FALSE}
nonprob(
  selection = ~ x1 + x2 + ... + xk, 
  outcome = y ~ x1 + x2 + ... + xk, 
  data = nonprob, 
  svydesign = prob, 
  method_outcome = "glm", 
  family_outcome = "gaussian",
  method_selection = "logit"
)
```

In this case the \texttt{target} is not needed as we define \texttt{selection} and \texttt{outcome} arguments. We also provide details on mass imputation and propensity score models to obtain a doubly robust estimator. Importantly, arguments such as \texttt{method\_outcome}, \texttt{method\_selection} or \texttt{family\_outcome} (and a few others) take default values described in more detail in the package documentation.
According to the description of the control functions, we can enforce that the estimation using the DR method is preceded by variable selection using the SCAD method for the IPW part and the MCP method for the MI part.


```{r, eval=FALSE}
nonprob(
  selection = ~ x1 + x2 + ... + xk, 
  outcome = y ~ x1 + x2 + ... + xk, 
  data = nonprob, 
  svydesign = prob, 
  method_outcome = "glm", 
  family_outcome = "gaussian",
  method_selection = "logit",
  control_selection = controlSel(penalty = "SCAD"),
  control_outcome = controlSel(penalty = "MCP"),
  control_inference = controlInf(vars_selection = TRUE),
  verbose = TRUE
)
```

In the control function concerning the selection mechanism for the non-probabilistic sample, we can also choose the weighting estimation method. The default value mle can be changed to gee along with the appropriate $h$ function (corresponding to $h\left(\bx_i, \btheta\right) = \bx_i$).

```{r, eval=FALSE}
nonprob(
  selection = ~ x1 + x2 + ... + xk, 
  outcome = y ~ x1 + x2 + ... + xk, 
  data = nonprob, 
  svydesign = prob, 
  method_outcome = "glm", 
  family_outcome = "gaussian",
  method_selection = "logit",
  control_selection = controlSel(est_method_sel = "gee", h = 2))
```

Mass imputation methods are defined in the argument \texttt{method\_outcome}. In the control function, we can set the parameters of a given method, e.g., the number of nearest neighbours in the NN algorithm.

```{r, eval=FALSE}
nonprob(
  selection = ~ x1 + x2 + ... + xk, 
  outcome = y ~ x1 + x2 + ... + xk, 
  data = nonprob, 
  svydesign = prob, 
  method_outcome = "nn",
  family_outcome = "gaussian",
  method_selection = "logit",
  control_outcome = controlOut(k = 3)
)
```

As the final example, we want to perform variable selection using the SCAD method for the IPW part, the default method for the MI part (also SCAD), choose bias minimization as the parameter estimation method for the DR estimator, and set the variance calculation method to bootstrap.

```{r, eval=FALSE}
nonprob(
  selection = ~ x1 + x2 + ... + xk, 
  outcome = y ~ x1 + x2 + ... + xk, 
  data = nonprob, 
  svydesign = prob, 
  method_outcome = "glm", 
  family_outcome = "gaussian",
  method_selection = "logit",
  control_selection = controlSel(penalty = "SCAD"),
  control_inference = controlInf(vars_selection = TRUE,
                                 bias_correction = TRUE,
                                 var_method = "bootstrap"),
  verbose = TRUE
)
```

The result of the function call will be an object of the class \texttt{nonprobsvy} containing a list of elements related to the estimation, i.e. the value of the estimated parameters, the mean and its standard deviation. On such an object, we can call the \texttt{summary} method, familiar to users of the R language. The result will look like this

```{verbatim}
Call:
nonprob(data = nonprob_df, selection = ~x1 + x2 + x3 + x4, outcome = y30 ~ 
    x1 + x2 + x3 + x4, svydesign = svyprob, method_selection = "logit")

-------------------------
Estimated population mean: 9.374 with overall std.err of: 0.3955
And std.err for nonprobability and probability samples being respectively:
0.364 and 0.1546

95% Confidence interval for population mean:
    lower_bound upper_bound
y30    8.598809    10.14916

Based on: Doubly-Robust method
For a population of estimate size: 20200.83
Obtained on a nonprobability sample of size: 950
With an auxiliary probability sample of size: 1001
-------------------------

Regression coefficients:
-----------------------
For glm regression on outcome variable:
            Estimate Std. Error z value  P(>|z|)    
(Intercept) -0.44155    1.05815  -0.417 0.676469    
x1           1.20976    0.72852   1.661 0.096802 .  
x2           1.50153    0.44714   3.358 0.000785 ***
x3           1.35890    0.28206   4.818 1.45e-06 ***
x4           1.17264    0.08029  14.606  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

-----------------------
For glm regression on selection variable:
             Estimate Std. Error z value  P(>|z|)    
(Intercept) -4.480634   0.113868 -39.349  < 2e-16 ***
x1          -0.028191   0.074889  -0.376    0.707    
x2           0.277772   0.044721   6.211 5.26e-10 ***
x3           0.148772   0.029746   5.001 5.69e-07 ***
x4           0.172832   0.008622  20.046  < 2e-16 ***
-------------------------

Weights:
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  1.143  11.180  19.502  21.264  28.089  79.119 
-------------------------

Covariate balance:
(Intercept)          x1          x2          x3          x4 
   72.18047  -425.80885  -584.02397  -351.61222  1508.57831 
-------------------------

Residuals:
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-0.31329 -0.04205 -0.01799  0.42335  0.94734  0.98736 

AIC: 7255.979
BIC: 7283.86
Log-Likelihood: -3622.99 on 1946 Degrees of freedom
```

# Practical examples
We apply our methods to integrate administrative and survey data about job vacancies in Poland. The goal is to estimate the percentage of single shift job vacancies according to available data sources. We defined our outcome variable $Y$ as follows: \textit{whether the vacancy notice was for single-shift work}. Now we present the description of data used.

The first source is the Job Vacancy Survey (JVS, also known as the Labour Demand Survey) with a sample of 6523 units. The data include "the number of employed persons, as well as the number and structure of job vacancies, including newly created and vacant positions reported to employment offices. Information on newly created and eliminated jobs" (Central Statistical Office of Poland). The survey is conducted using a representative method. The sample is drawn separately for units employing more than 9 people and for units employing up to 9 people. The sampling frame for this survey is the Statistical Units Database.
It includes information about NACE (The Nomenclature of Economic Activities) (19 levels), region (16 levels), sector (2 levels), size (3 levels) and the number of employees according to administrative data integrated by Statistics Poland (RE). 

```{r, echo=FALSE, message=FALSE}
library(nonprobsvy)
```

```{r, out.width="80%"}
jvs <- read.csv("data-raw/jvs.csv",
                  colClasses = c("character", "numeric",
                                 rep("character", 3), "numeric"))
head(jvs)
```

```{r}
jvs_svy <- svydesign(ids = ~ 1, 
                     weights = ~ weight,
                     strata = ~ size + nace + region,
                     data = jvs)
svytotal(~size, jvs_svy)
```

The second source is the Central Job Offers Database (CBOP), which is a register of all vacancies submitted to Public Employment Offices and can be accessed via CBOP API. It contains job offers submitted by employers looking for new employees. If an employer is seeking new workers for their business, they can approach the County Employment Office (PUP) and submit the appropriate application. CBOP also
contains information about unit identifiers (REGON and NIP), so we were able to link units to the sampling frame to obtain auxiliary variables with the same definitions as those used
in the survey. Beyond that it contains \texttt{single\_shift} outcome variable.

```{r}
admin <- read.csv("data-raw/admin.csv",
                 colClasses = c("character", "numeric",
                                rep("character", 3), "logical")
                 )
head(admin)
```

## IPW 

```{r}
est1_logit <- nonprob(
  selection = ~ region + private + nace + size,
  target = ~ single_shift,
  svydesign = jvs_svy,
  data = admin,
  method_selection = "logit"
)
```

```{r}
summary(est1_logit)
```

## IPW CALIB
```{r}
est2_logit <- nonprob(
  selection = ~ region + private + nace + size,
  target = ~ single_shift,
  svydesign = jvs_svy,
  data = admin,
  method_selection = "logit",
  control_selection = controlSel(h = 1, est_method_sel = "gee")
)
```

## IPW bootstrap
```{r}
est3_logit <- nonprob(
  selection = ~ region + private + nace + size,
  target = ~ single_shift,
  svydesign = jvs_svy,
  data = admin,
  method_selection = "logit",
  control_inference = controlInf(var_method = "bootstrap", num_boot = 50),
  verbose = F,
)
```

## MI

```{r}
est5_glm <- nonprob(
  outcome = single_shift ~ region + private + nace + size,
  svydesign = jvs_svy,
  data = admin,
  method_outcome = "glm",
  family_outcome = "gaussian"
)
```

## DR
```{r}
est8_dr1 <- nonprob(
  selection = ~ region + private + nace + size,
  outcome = single_shift ~ region + private + nace + size,
  svydesign = jvs_svy,
  data = admin,
  method_selection = "logit",
  method_outcome = "glm",
  family_outcome = "binomial",
  pop_size = sum(weights(jvs_svy))
)
```

# Classes and S3methods


# Summary and future work

The `nonprobsvy` package is an R tool developed to address challenges associated with making inferences from non-probability samples. As non-probability data sources such as administrative data, voluntary online panels, and social media data become increasingly available, statisticians are faced with the problem of how to integrate these sources with traditional probability samples to produce reliable population estimates. This package provides a suite of methods specifically designed to handle the unique characteristics of non-probability samples, where selection mechanisms are often unknown or biased.

`nonprobsvy` supports a range of estimation techniques including mass imputation, inverse probability weighting (IPW), and doubly robust (DR) methods. Each of these methods allows researchers to correct for selection bias in non-probability samples by leveraging auxiliary data from probability samples or known population totals. The mass imputation approach imputes values for the outcome variable in the probability sample using models trained on the non-probability sample. The IPW estimator uses estimated propensity scores to weight non-probability samples in a way that aligns them more closely with the population. The DR estimator combines both approaches, resulting in an estimator that remains consistent if either the outcome model or the propensity score model is correctly specified, thus providing a level of robustness against model misspecification.

Designed to integrate with the widely used `survey` package in R, `nonprobsvy` enables researchers to define survey designs, specify selection and outcome models, and estimate population parameters using both probability and non-probability samples. The package supports advanced modeling options, including generalized linear models, nearest neighbor algorithms, and predictive mean matching, and allows for customization of tuning parameters, variable selection, and variance estimation methods (e.g., bootstrap).

Overall, `nonprobsvy` is a comprehensive toolkit for researchers and practitioners working with mixed data sources. It simplifies the implementation of complex statistical methods for non-probability samples, making it easier to produce robust, reliable estimates even when working with non-representative data. By providing a unified framework for inference with non-probability samples, `nonprobsvy` contributes to the growing field of data integration and enhances the ability of statisticians to make informed decisions from diverse data sources.

A natural extension of this study is that consistency can be maintained when the design weights $d_i$ are replaced by calibration weights, which is often the case when working with survey sample datasets. In addition, other methods of estimating propensity scores proposed in the literature can be considered, such as estimation using the empirical likelihood approach. It is also worth exploring the situation where the two probability and non-probability samples overlap, i.e. there are units present in both datasets. This type of problem will be the focus of the author's future work on non--probability sampling, in particular its integration with other statistical data sources.


